# Modelfile for RAG-trained fine-tuned model (trained WITH RAG contexts)
# Use this for: BOTH Condition 5 (RAG-trained FT Only) AND Condition 6 (RAG-trained FT+RAG)
#   - Condition 5: RAG-trained FT Only (evaluated WITHOUT RAG contexts)
#   - Condition 6: RAG-trained FT+RAG (evaluated WITH RAG contexts - optimal setup)
#
# Convert your merged Hugging Face checkpoint to GGUF (e.g., llama.cpp `convert`
# and `quantize`) and point FROM at the resulting file.
#
# This model was trained with RAG contexts, so it expects and can cite retrieved contexts.

# Replace the relative path below with the GGUF you export from the merged adapter.
# Example: outputs/lora_science_v1/merged_full_model/Qwen2.5-0.5B-lora_science_v1.Q4_K_M.gguf
FROM ../outputs/lora_science_v1/merged_full_model/Qwen2.5-0.5B-lora_science_v1.Q4_K_M.gguf

PARAMETER temperature 0
PARAMETER top_p 0.9
PARAMETER repeat_penalty 1.05
PARAMETER num_ctx 4096
PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"

TEMPLATE """
{{- if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{- end }}{{- range .Messages }}<|im_start|>{{ .Role }}
{{ .Content }}<|im_end|>
{{- end }}<|im_start|>assistant
"""

# System prompt: Matches training system message for consistency
# Training uses: "You are a scientific research assistant who answers with concise,
# evidence-grounded prose and includes inline numeric citations like [1], [2], etc."
# This ensures evaluation matches training conditions exactly
SYSTEM "You are a scientific research assistant who answers with concise, evidence-grounded prose and includes inline numeric citations like [1], [2], etc."
