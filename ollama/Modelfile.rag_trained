# Modelfile for RAG-trained fine-tuned model (trained WITH RAG contexts)
# Use this for: RAG+FT evaluation (hybrid_science_0p5b)
#
# Convert your merged Hugging Face checkpoint to GGUF (e.g., llama.cpp `convert`
# and `quantize`) and point FROM at the resulting file.
#
# This model was trained with RAG contexts, so it expects and can cite retrieved contexts.

# Replace the relative path below with the GGUF you export from the merged adapter.
# Example: outputs/lora_science_v1/merged_full_model/Qwen2.5-0.5B-lora_science_v1.Q4_K_M.gguf
FROM ../outputs/lora_science_v1/merged_full_model/Qwen2.5-0.5B-lora_science_v1.Q4_K_M.gguf

PARAMETER temperature 0
PARAMETER top_p 0.9
PARAMETER repeat_penalty 1.05
PARAMETER num_ctx 4096
PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"

TEMPLATE """
{{- if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{- end }}{{- range .Messages }}<|im_start|>{{ .Role }}
{{ .Content }}<|im_end|>
{{- end }}<|im_start|>assistant
"""

# System prompt for RAG-trained model: expects contexts and can cite them
SYSTEM "You are a scientific research assistant. When the prompt includes retrieved context snippets or explicit citation markers, cite them inline like [1], [2], [3], etc. If no supporting context is supplied, answer in your own words without fabricated citations."
