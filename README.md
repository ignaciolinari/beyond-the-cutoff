# Beyond the Cutoff

A reproducible pipeline for evaluating how language models acquire knowledge beyond their training data cutoff, comparing **fine-tuning** and **retrieval-augmented generation (RAG)** approaches.

> **ğŸ“Š Experiment Status: COMPLETED** â€” This pipeline was tested on Qwen 2.5 0.5B (a small language model). Results and methodology are documented below. The pipeline is model-agnostic and can be adapted for larger models.

## Overview

This project provides an end-to-end framework for investigating how LLMs can acquire new scientific knowledge published after their training cutoff. We compare two adaptation strategies:

| Strategy | Approach | Pros | Cons |
|----------|----------|------|------|
| **Fine-tuning** | Update model weights with new data | Internalizes knowledge | Expensive, may forget |
| **RAG** | Retrieve relevant context at inference | No retraining needed | Depends on retrieval quality |

The benchmark uses **2025 arXiv papers** (post-cutoff for current open-weight models) as the knowledge source.

### Completed Experiment

| Aspect | Configuration |
|--------|---------------|
| **Subject Model** | Qwen 2.5 0.5B Instruct (small language model, Q4_K_M quantized) |
| **Judge Model** | Qwen 3 8B (thinking mode) + Gemini 3 Pro (pairwise) |
| **Dataset** | 120 arXiv papers (2025), 154 evaluation QA pairs |
| **Quantization** | Q4_K_M (4-bit) vs F16 comparison |

> âš ï¸ **Scope**: The findings below apply to **Qwen 2.5 0.5B** â€” a small language model with limited capacity. Larger models (3B, 7B, 14B+) may exhibit different patterns, particularly regarding fine-tuning benefits. This pipeline is designed to test such hypotheses.

---

## Experimental Results (Qwen 2.5 0.5B)

### 1. Six-Condition Experiment

We evaluated 6 conditions crossing training approach (none, instruction-only FT, RAG-trained FT) with inference mode (with/without RAG):

| Finding | Evidence |
|---------|----------|
| **RAG provides substantial improvement** | 4-5x improvement in pass rate (22% vs 4%) |
| **Fine-tuning provides marginal gains** | +1.3% when training format matches inference |
| **Training format affects transfer** | RAG-trained models perform better with RAG at inference |
| **Fine-tuning alone insufficient** | FT-only models (no RAG) perform similarly to base (4-6%) |

### 2. Pairwise Comparison (Top 2 Candidates)

| Comparison | Result |
|------------|--------|
| **Base+RAG vs FT+RAG** | 54.9% vs 45.1% win rate (p=0.35, **not statistically significant**) |
| **Judge** | Gemini 3 pro (62 FT wins, 51 Base wins, 41 ties) |
| **Interpretation** | Insufficient evidence that fine-tuning improves RAG performance for this model size |

### 3. Quantization Impact (Q4_K_M vs F16)

| Metric | Result |
|--------|--------|
| **Win rate** | 48.4% vs 51.6% (p=0.84, **no significant difference**) |
| **Size reduction** | 60% smaller (397MB vs 994MB) |
| **Conclusion** | Q4_K_M quantization does not degrade quality for this model |

### Summary

For **Qwen 2.5 0.5B** on post-cutoff scientific QA:

1. **RAG is essential** â€” provides the primary performance gain
2. **Fine-tuning has minimal impact** â€” Base+RAG achieves ~95% of best performance
3. **Q4_K_M is viable** â€” no measurable quality loss vs F16
4. **Open question** â€” larger models may show different fine-tuning benefits

ğŸ“– **Detailed reports**: [Six-Condition Results](docs/reports/six_condition_experiment_results.md) | [Pairwise Evaluation](docs/reports/pairwise_evaluation_results.md) | [Quantization Analysis](docs/reports/quantization_comparison_results.md)

---

## Pipeline Overview

The project consists of 5 main stages:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. FETCH   â”‚â”€â”€â”€â–¶â”‚  2. INGEST  â”‚â”€â”€â”€â–¶â”‚ 3. DATASET  â”‚â”€â”€â”€â–¶â”‚ 4. FINETUNE â”‚â”€â”€â”€â–¶â”‚ 5. EVALUATE â”‚
â”‚   Papers    â”‚    â”‚   & Index   â”‚    â”‚  Generate   â”‚    â”‚   Models    â”‚    â”‚   Compare   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    arXiv           PDFâ†’Text           QA/Summary         LoRA/PEFT          6 Conditions
    Harvest         FAISS Index        Train/Eval         2 Models           LLM Judge
```

### Stage 1: Fetch Papers
Download recent arXiv papers that are beyond model training cutoffs.

```bash
python scripts/data/fetch_arxiv_corpus.py \
    --contact-email you@example.com \
    --total 100 \
    --output-dir data/raw/arxiv_2025
```

### Stage 2: Ingest & Index
Convert PDFs to text, chunk documents, and build a FAISS retrieval index.

```bash
python scripts/data/ingest_and_index.py --config configs/default.yaml
```

Output: `data/processed/` (text), `data/external/index/` (FAISS)

### Stage 3: Generate Dataset
Create QA pairs, summaries, and citation tasks using a strong generator model.

```bash
# Generate offline dataset
python scripts/data/generate_offline_dataset.py --config configs/default.yaml

# Validate quality with LLM judge
python scripts/data/evaluate_dataset_quality.py \
    --dataset evaluation/datasets/offline_dataset.jsonl

# Split into train/eval sets
python scripts/data/split_dataset.py \
    --input evaluation/datasets/offline_dataset.jsonl \
    --train-output evaluation/datasets/train_dataset.jsonl \
    --eval-output evaluation/datasets/eval_dataset.jsonl
```

### Stage 4: Fine-tune Models
Train two LoRA models in cloud notebooks (Colab/Kaggle):

| Model | Notebook | Training Data |
|-------|----------|---------------|
| **Instruction-only** | `notebooks/finetuning/lora_science_v1_instruction_only.ipynb` | Without RAG contexts |
| **RAG-trained** | `notebooks/finetuning/lora_science_v1.ipynb` | With RAG contexts |

Register with Ollama after training:
```bash
ollama create lora_science_0p5_instruction_only -f ollama/Modelfile.instruction_only
ollama create lora_science_0p5 -f ollama/Modelfile.rag_trained
```

### Stage 5: Evaluate (6-Condition Experiment)

| # | Condition | Training | RAG at Eval | Purpose |
|---|-----------|----------|-------------|---------|
| 1 | Base Baseline | None | âŒ | Lower bound |
| 2 | RAG Baseline | None | âœ… | RAG-only benefit |
| 3 | FT Only (instruction) | Instruction-only | âŒ | FT-only benefit |
| 4 | FT+RAG (instruction) | Instruction-only | âœ… | Transfer learning test |
| 5 | FT Only (RAG-trained) | RAG-trained | âŒ | Degradation test |
| 6 | FT+RAG (RAG-trained) | RAG-trained | âœ… | Optimal setup |

```bash
python scripts/core/compare_models.py --plan configs/evaluation/six_condition_experiment.yaml
```

ğŸ“– See [docs/experiment/](docs/experiment/) for detailed methodology.

## Quick Start

### Prerequisites

- Python â‰¥ 3.10
- [Ollama](https://ollama.com/) for local inference
- macOS (Apple Silicon recommended) or Linux

### Installation

```bash
git clone https://github.com/ignaciolinari/beyond-the-cutoff.git
cd beyond-the-cutoff
python scripts/bootstrap_env.py
source .venv/bin/activate

# Pull required models
ollama pull qwen2.5:0.5b-instruct   # Base model
ollama pull qwen3:8b                 # Judge model
```

### Interactive RAG Assistant

Once you have papers ingested, ask questions:

```bash
python scripts/ask.py "What are the main contributions of paper X?"
```

## Project Structure

```
beyond-the-cutoff/
â”œâ”€â”€ apps/                      # Interactive Streamlit applications
â”‚   â”œâ”€â”€ human_annotation.py    # Human evaluation annotation UI
â”‚   â””â”€â”€ offline_task_viewer.py # Browse generated QA tasks
â”œâ”€â”€ artifacts/                 # Training artifacts and checkpoints
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ default.yaml          # Main pipeline configuration
â”‚   â”œâ”€â”€ models/               # Model configurations (base, fine-tuned)
â”‚   â”œâ”€â”€ evaluation/           # Experiment plans (6-condition, pairwise, etc.)
â”‚   â””â”€â”€ judges/               # LLM judge configurations
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Downloaded PDFs from arXiv
â”‚   â”œâ”€â”€ processed/            # Extracted text and manifests
â”‚   â””â”€â”€ external/             # FAISS index and embeddings
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ experiment/           # Experiment design and methodology
â”‚   â”œâ”€â”€ reports/              # Analysis results and findings
â”‚   â”œâ”€â”€ reference/            # Technical documentation
â”‚   â”œâ”€â”€ scaling/              # Guide for larger models
â”‚   â””â”€â”€ future/               # Planned features
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ datasets/             # Train/eval JSONL datasets
â”‚   â”œâ”€â”€ responses/            # Pre-generated model responses
â”‚   â”œâ”€â”€ results/              # Evaluation metrics and details
â”‚   â””â”€â”€ exports/              # Batch files for external judges
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ finetuning/           # LoRA training notebooks (Colab/Kaggle)
â”‚   â””â”€â”€ data_quality/         # Data analysis notebooks
â”œâ”€â”€ ollama/                   # Modelfiles for Ollama registration
â”œâ”€â”€ outputs/                  # Fine-tuned model weights (GGUF)
â”œâ”€â”€ prompts/                  # Prompt templates
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ask.py                # Interactive RAG assistant
â”‚   â”œâ”€â”€ bootstrap_env.py      # Environment setup
â”‚   â”œâ”€â”€ core/                 # Main evaluation pipeline
â”‚   â”‚   â”œâ”€â”€ compare_models.py
â”‚   â”‚   â”œâ”€â”€ generate_responses.py
â”‚   â”‚   â””â”€â”€ interleaved_evaluation.py
â”‚   â”œâ”€â”€ data/                 # Data processing scripts
â”‚   â”‚   â”œâ”€â”€ fetch_arxiv_corpus.py
â”‚   â”‚   â”œâ”€â”€ ingest_and_index.py
â”‚   â”‚   â”œâ”€â”€ generate_offline_dataset.py
â”‚   â”‚   â””â”€â”€ split_dataset.py
â”‚   â”œâ”€â”€ future/               # Advanced evaluation (ELO, retrieval ablation)
â”‚   â”œâ”€â”€ utility/              # Analysis and inspection tools
â”‚   â””â”€â”€ validation/           # Experiment validation scripts
â”œâ”€â”€ src/beyond_the_cutoff/    # Core Python library
â”‚   â”œâ”€â”€ data/                 # PDF loading, chunking, extraction
â”‚   â”œâ”€â”€ retrieval/            # FAISS indexing and querying
â”‚   â”œâ”€â”€ evaluation/           # Judges, metrics, scoring
â”‚   â”œâ”€â”€ datasets/             # Dataset generation
â”‚   â””â”€â”€ models/               # Model interfaces
â”œâ”€â”€ tests/                    # Pytest test suite
â””â”€â”€ vintage/                  # Archived configs and scripts
```

## Documentation

| Section | Description |
|---------|-------------|
| [ğŸ“‹ Experiment Setup](docs/experiment/setup.md) | 6-condition design |
| [ğŸ“Š Methodology](docs/experiment/methodology.md) | Evaluation metrics |
| [ğŸ”§ Pipeline Reference](docs/reference/pipeline.md) | Full technical details |

See [docs/README.md](docs/README.md) for the complete documentation index.

## Models

| Model | Purpose | Ollama Tag |
|-------|---------|------------|
| Qwen 2.5 0.5B | Base model for experiments | `qwen2.5:0.5b-instruct` |
| Qwen 3 8B | LLM judge | `qwen3:8b` |
| Fine-tuned (instruction) | Trained without RAG contexts | `lora_science_0p5_instruction_only` |
| Fine-tuned (RAG-trained) | Trained with RAG contexts | `lora_science_0p5` |

## Development

```bash
pytest tests/              # Run tests
pre-commit run --all-files # Linting & formatting
mypy src/                  # Type checking
```

---

## Adapting the Pipeline

This pipeline is **model-agnostic**. To test with different models:

### Changing the Subject Model

| Model Size | Ollama Tag | Notes |
|------------|------------|-------|
| **3B** | `qwen2.5:3b-instruct` | Moderate capacity increase |
| **7B** | `qwen2.5:7b-instruct-q4_K_M` | ~11GB VRAM |
| **14B+** | `qwen2.5:14b-instruct-q4_K_M` | ~16GB VRAM |

1. Update `configs/models/base_ollama.yaml` with the new model tag
2. Fine-tune using the notebooks (adjust for model size)
3. Re-run the evaluation pipeline

### Hypotheses for Larger Models

| Aspect | 0.5B (Observed) | Larger Models (Hypothesis) |
|--------|-----------------|----------------------------|
| **FT benefit** | Marginal (+1.3%) | May increase with capacity |
| **Knowledge retention** | Limited | Better with more parameters |
| **RAG dependence** | Critical | May decrease |

ğŸ“– See [docs/scaling/](docs/scaling/) for detailed instructions.

---

## Future Work

Features implemented but not executed in this experiment:

| Feature | Script | Purpose |
|---------|--------|---------|
| **Live retrieval evaluation** | `scripts/future/evaluate_end_to_end.py` | Test with dynamic retrieval |
| **Retrieval ablation** | `scripts/future/run_retrieval_ablation.py` | Optimize top_k and rerankers |
| **ELO tournament** | `scripts/future/compute_elo_rankings.py` | Multi-model ranking |

See [docs/future/](docs/future/) and [scripts/future/README.md](scripts/future/README.md).

## License

MIT License. See [LICENSE](LICENSE) for details.
