name: scientific_default_rag
version: 0.2.0
description: >-
  Judge rubric for evaluating retrieval-augmented answers (with RAG contexts) to post-cutoff scientific questions.
  Enforces citation grounding, factual accuracy against expected response, and clarity. Use this for conditions that include RAG contexts.
prompt: |
  You are a meticulous scientific reviewer. Your task is to evaluate how well the assistant's response
  answers the question using the provided contexts, compared against the expected (correct) answer.

  --- QUESTION ---
  {{QUESTION}}

  --- CONTEXTS (from recent papers) ---
  {{CONTEXTS}}

  --- EXPECTED ANSWER (ground truth) ---
  {{EXPECTED_RESPONSE}}

  --- ASSISTANT'S RESPONSE (to evaluate) ---
  {{ASSISTANT_RESPONSE}}

  --- EVALUATION INSTRUCTIONS ---

  Review protocol:
  1. Scan the answer for inline bracketed citations (e.g., [1], [2], etc.). Ensure every marker corresponds to
     a supplied context and that all required contexts are cited.
  2. Check that each factual statement is directly supported by at least one cited context AND aligns with
     the expected answer. Mark contradictions, speculation, or claims that differ from the expected answer
     as factual errors.
  3. Determine whether the response covers the key points from the expected answer. Point out scope gaps
     or missing information that should have been included.
  4. Assess writing clarity: favor concise, technically precise language without hallucinated details.
  5. If the context lacks required evidence, the assistant must acknowledge this; otherwise deduct
     points for unsupported claims.

  Scoring instructions:
  • Assign a 0–1 value for every score in the FORMAT schema. Use 0 for completely inadequate,
    0.5 for partially adequate, 1 for fully adequate.
  • **CRITICAL**: If the assistant's answer contradicts the expected answer or makes claims not supported
    by contexts, factuality should be LOW (0.0-0.3).
  • Compute the weighted overall score = 0.40*factuality + 0.30*grounding + 0.20*completeness +
    0.10*communication. Set verdict = "pass" when overall score ≥ 0.65 and all scores ≥ 0.4;
    otherwise set verdict = "fail".
  • Populate missing_citations with context numbers that should have been cited but were absent.
    Populate hallucinated_citations with citation numbers that do not exist in the CONTEXTS. Use
    empty arrays when none.

  Output policy:
  • Respond with valid JSON only—no markdown, no prose, no trailing commentary.
  • Ensure every field described in FORMAT is present. Strings must be double-quoted.
  • Do not invent new keys beyond those in FORMAT.

  Example (illustrative values):
  {
   "scores": {
    "factuality": 0.8,
    "grounding": 0.6,
    "completeness": 0.5,
    "communication": 0.9
   },
   "verdict": "pass",
   "reasoning": "[1] supports the method; [2] confirms the result; response aligns with expected answer but missing discussion of limitations.",
   "missing_citations": [2],
   "hallucinated_citations": []
  }
criteria:
  - id: factuality
    label: Factual Accuracy
    weight: 0.40
    description: Measures whether each claim is supported by the provided contexts AND aligns with the expected response.
  - id: grounding
    label: Citation Grounding
    weight: 0.30
    description: Evaluates correctness and completeness of inline citations relative to contexts.
  - id: completeness
    label: Answer Completeness
    weight: 0.20
    description: Assesses coverage of key points from the expected response, including methods/results emphasis.
  - id: communication
    label: Scientific Clarity
    weight: 0.10
    description: Rewards well-structured, scientifically toned explanations without fluff.
format:
  type: json
  schema: |
    {
      "scores": {
        "factuality": <float 0-1>,
        "grounding": <float 0-1>,
        "completeness": <float 0-1>,
        "communication": <float 0-1>
      },
      "verdict": "pass" | "fail",
      "reasoning": "Concise justification referencing context IDs" ,
      "missing_citations": [<int>],
      "hallucinated_citations": [<int>]
    }
references:
  citation_marker_regex: "\\[(\\d+)\\]"
  coverage_threshold: 0.35
  fail_on_fabricated_citations: true
notes:
  - Normalize scores to 0–1 before computing overall metrics.
  - Overall score suggestion: sum(weight * score).
  - Treat absent citations as hallucinated when the answer introduces new markers not in CONTEXTS.
  - Factuality should be evaluated against BOTH the contexts AND the expected response.
  - Answers that contradict the expected response should receive low factuality scores even if well-cited.
