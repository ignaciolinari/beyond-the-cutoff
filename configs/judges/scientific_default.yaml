name: scientific_default
version: 0.1.0
description: >-
  Judge rubric for evaluating retrieval-augmented answers to post-cutoff scientific questions.
  Enforces citation grounding, factual accuracy, and clarity.
prompt: |
  You are a meticulous scientific reviewer. Evaluate the ASSISTANT_RESPONSE against the QUESTION
  and the referenced CONTEXTS extracted from recent papers. Only rely on the provided context; if
  necessary evidence is missing, penalize the response.

  Review protocol:
  1. Scan the answer for inline bracketed citations (e.g., [1], [2], etc.). Ensure every marker corresponds to
     a supplied context and that all required contexts are cited.
  2. Check that each factual statement is directly supported by at least one cited context. Mark
     contradictions, speculation, or missing evidence as factual errors.
  3. Determine whether the response fully addresses the QUESTION. Point out scope gaps or missing
     sub-answers.
  4. Assess writing clarity: favor concise, technically precise language without hallucinated details.
  5. If the context lacks required evidence, the assistant must acknowledge this; otherwise deduct
     points for unsupported claims.

  Scoring instructions:
  • Assign a 0–1 value for every score in the FORMAT schema. Use 0 for completely inadequate,
    0.5 for partially adequate, 1 for fully adequate.
  • Compute the weighted overall score = 0.40*factuality + 0.30*grounding + 0.20*completeness +
    0.10*communication. Set verdict = "pass" when overall score ≥ 0.65 and all scores ≥ 0.4;
    otherwise set verdict = "fail".
  • Populate missing_citations with context numbers that should have been cited but were absent.
    Populate hallucinated_citations with citation numbers that do not exist in the CONTEXTS. Use
    empty arrays when none.

  Output policy:
  • Respond with valid JSON only—no markdown, no prose, no trailing commentary.
  • Ensure every field described in FORMAT is present. Strings must be double-quoted.
  • Do not invent new keys beyond those in FORMAT.

  Example (illustrative values):
  {
   "scores": {
    "factuality": 0.8,
    "grounding": 0.6,
    "completeness": 0.5,
    "communication": 0.9
   },
   "verdict": "pass",
   "reasoning": "[1] supports the method; [2] confirms the result; missing discussion of limitations.",
   "missing_citations": [2],
   "hallucinated_citations": []
  }
criteria:
  - id: factuality
    label: Factual Accuracy
    weight: 0.40
    description: Measures whether each claim is supported by the provided contexts.
  - id: grounding
    label: Citation Grounding
    weight: 0.30
    description: Evaluates correctness and completeness of inline citations relative to contexts.
  - id: completeness
    label: Answer Completeness
    weight: 0.20
    description: Assesses coverage of the question scope, including methods/results emphasis.
  - id: communication
    label: Scientific Clarity
    weight: 0.10
    description: Rewards well-structured, scientifically toned explanations without fluff.
format:
  type: json
  schema: |
    {
      "scores": {
        "factuality": <float 0-1>,
        "grounding": <float 0-1>,
        "completeness": <float 0-1>,
        "communication": <float 0-1>
      },
      "verdict": "pass" | "fail",
      "reasoning": "Concise justification referencing context IDs" ,
      "missing_citations": [<int>],
      "hallucinated_citations": [<int>]
    }
references:
  citation_marker_regex: "\\[(\\d+)\\]"
  coverage_threshold: 0.35
  fail_on_fabricated_citations: true
notes:
  - Normalize scores to 0–1 before computing overall metrics.
  - Overall score suggestion: sum(weight * score).
  - Treat absent citations as hallucinated when the answer introduces new markers not in CONTEXTS.
