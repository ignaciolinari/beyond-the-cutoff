name: scientific_default_instruction
version: 0.2.0
description: >-
  Judge rubric for evaluating instruction-only answers (no RAG contexts) to post-cutoff scientific questions.
  Evaluates factuality against the expected response (ground truth), completeness, and clarity. Citation checks are skipped.
prompt: |
  You are a meticulous scientific reviewer. Your task is to evaluate how well the assistant's response
  matches the expected (correct) answer.

  **IMPORTANT**: No contexts are provided for this evaluation. The assistant answered based on its general knowledge only.
  Do NOT check for citations or grounding against contexts—these are not applicable in instruction-only mode.

  --- QUESTION ---
  {{QUESTION}}

  --- EXPECTED ANSWER (ground truth) ---
  {{EXPECTED_RESPONSE}}

  --- ASSISTANT'S RESPONSE (to evaluate) ---
  {{ASSISTANT_RESPONSE}}

  --- EVALUATION INSTRUCTIONS ---

  Review protocol:
  1. **Factuality**: Compare the assistant's response to the expected answer above. Check whether the assistant's
     factual claims align with the ground truth. Mark contradictions, incorrect information, or claims that
     differ from the expected answer as factual errors. A response that confidently states incorrect information
     (hallucination) should receive a LOW factuality score, even if it sounds plausible.
  2. **Completeness**: Determine whether the response covers the key points from the expected answer.
     Point out scope gaps, missing sub-answers, or information present in the expected answer but absent
     from the assistant's response.
  3. **Communication**: Assess writing clarity: favor concise, technically precise language without
     hallucinated details or unnecessary verbosity.
  4. **Citation/Grounding**: SKIP this entirely. Do not penalize for missing citations or lack of context
     grounding—these are not applicable when no contexts are provided.

  Scoring instructions:
  • Assign a 0–1 value for every score in the FORMAT schema. Use 0 for completely inadequate,
    0.5 for partially adequate, 1 for fully adequate.
  • **CRITICAL**: If the assistant's answer contradicts or is completely unrelated to the expected answer,
    factuality should be 0.0-0.3. If it partially matches, use 0.4-0.6. If it closely matches, use 0.7-1.0.
  • Compute the weighted overall score = 0.50*factuality + 0.30*completeness + 0.20*communication.
    Note: grounding is set to 0.0 (not applicable) and excluded from the overall score.
  • Set verdict = "pass" when overall score ≥ 0.65 and factuality ≥ 0.4; otherwise set verdict = "fail".
  • Set missing_citations and hallucinated_citations to empty arrays (not applicable).

  Output policy:
  • Respond with valid JSON only—no markdown, no prose, no trailing commentary.
  • Ensure every field described in FORMAT is present. Strings must be double-quoted.
  • Do not invent new keys beyond those in FORMAT.

  Example (illustrative values):
  {
   "scores": {
    "factuality": 0.8,
    "grounding": 0.0,
    "completeness": 0.6,
    "communication": 0.9
   },
   "verdict": "pass",
   "reasoning": "Response aligns with expected answer on main points but misses specific details about methodology.",
   "missing_citations": [],
   "hallucinated_citations": []
  }
criteria:
  - id: factuality
    label: Factual Accuracy (vs Expected Response)
    weight: 0.50
    description: Measures whether assistant's claims match the expected/reference answer. Hallucinated or contradictory claims score low.
  - id: grounding
    label: Citation Grounding
    weight: 0.00
    description: Not applicable for instruction-only evaluation. Always set to 0.0.
  - id: completeness
    label: Answer Completeness
    weight: 0.30
    description: Assesses coverage of key points from the expected response.
  - id: communication
    label: Scientific Clarity
    weight: 0.20
    description: Rewards well-structured, scientifically toned explanations without fluff.
format:
  type: json
  schema: |
    {
      "scores": {
        "factuality": <float 0-1>,
        "grounding": <float 0-1, always 0.0 for instruction-only>,
        "completeness": <float 0-1>,
        "communication": <float 0-1>
      },
      "verdict": "pass" | "fail",
      "reasoning": "Concise justification focusing on factuality and completeness",
      "missing_citations": [],
      "hallucinated_citations": []
    }
references:
  citation_marker_regex: "\\[(\\d+)\\]"
  coverage_threshold: 0.35
  fail_on_fabricated_citations: false
notes:
  - This prompt is for instruction-only evaluation (no RAG contexts).
  - Grounding score should always be 0.0 (not applicable).
  - Citation arrays should always be empty (not applicable).
  - Factuality is evaluated by comparing to the EXPECTED_RESPONSE (ground truth).
  - Hallucinated answers that don't match expected response should score LOW on factuality.
  - Overall score excludes grounding: 0.50*factuality + 0.30*completeness + 0.20*communication.
