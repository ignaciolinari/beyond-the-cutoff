project:
  name: "Beyond the Cutoff"
  seed: 42
paths:
  raw_data: data/raw
  processed_data: data/processed
  external_data: data/external
retrieval:
  vector_store: faiss
  embedding_model: BAAI/bge-small-en-v1.5
  chunk_size: 800
  chunk_overlap: 120
  top_k: 4
  max_context_chars: 6000
  chunking_strategy: sentences
  reranker_model: cross-encoder/ms-marco-MiniLM-L-6-v2
fine_tuning:
  base_model: HuggingFaceTB/SmolLM2-135M
  adapter_output_dir: outputs/adapters
  lora_rank: 16
  learning_rate: 0.0001
  batch_size: 4
  gradient_accumulation_steps: 4
  max_steps: 1000
evaluation:
  metrics:
    - factuality
    - citation_accuracy
    - bleu
    - bertscore
  qa_dataset_path: evaluation/datasets/qa_pairs.jsonl
  summary_dataset_path: evaluation/datasets/summaries.jsonl
inference:
  provider: transformers
  model: HuggingFaceTB/SmolLM2-135M
  device: auto
  torch_dtype: auto
  max_new_tokens: 512
  temperature: 0.1
  top_p: 0.95
  repetition_penalty: 1.05
  stop_sequences: []
  # When ready to graduate to a larger Phi checkpoint, switch to `provider: ollama`
  # (or another deployment target) and update the `model` tag accordingly. The
  # remaining parameters can stay as-is unless you need different sampling behavior.
