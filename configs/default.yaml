project:
  name: "Beyond the Cutoff"
  seed: 42
paths:
  raw_data: data/raw
  processed_data: data/processed
  external_data: data/external
retrieval:
  vector_store: faiss
  embedding_model: BAAI/bge-small-en-v1.5
  chunk_size: 800
  chunk_overlap: 120
  top_k: 4
  max_context_chars: 6000
  chunking_strategy: sentences
  reranker_model: cross-encoder/ms-marco-MiniLM-L-6-v2
fine_tuning:
  base_model: Qwen/Qwen2-0.5B-Instruct
  adapter_output_dir: outputs/adapters
  lora_rank: 16
  learning_rate: 0.0001
  batch_size: 4
  gradient_accumulation_steps: 4
  max_steps: 1000
evaluation:
  metrics:
    - factuality
    - citation_accuracy
    - bleu
    - bertscore
  qa_dataset_path: evaluation/datasets/qa_pairs.jsonl
  summary_dataset_path: evaluation/datasets/summaries.jsonl
  offline_tasks_path: evaluation/datasets/offline_tasks.jsonl
  offline_dataset_path: evaluation/datasets/offline_dataset.jsonl
dataset_generation:
  generator:
    provider: ollama
    model: qwen2:1.5b-instruct-q4_0
    host: http://localhost
    port: 11434
    timeout: 120.0
    device: auto  # retained for interface parity with transformers backend
    torch_dtype: auto
    max_new_tokens: 768
    temperature: 0.5
    top_p: 0.95
    repetition_penalty: 1.05
    stop_sequences: []
  questions_per_document: 4
  summary_prompts_per_document: 1
  citation_prompts_per_document: 1
  max_chunks_per_document: 6
  max_chars_per_chunk: 1600
  max_documents: null
  seed: 42
inference:
  provider: ollama
  model: qwen2:0.5b-instruct-q4_0
  host: http://localhost
  port: 11434
  timeout: 120.0
  device: auto
  torch_dtype: auto
  max_new_tokens: 512
  temperature: 0.0
  top_p: 0.9
  repetition_penalty: 1.05
  stop_sequences: []
  # Swap this tag to your fine-tuned adapter (or a larger checkpoint) once available.
