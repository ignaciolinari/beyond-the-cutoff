{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9dadf17",
   "metadata": {},
   "source": [
    "# Corpus Quality Checks\n",
    "\n",
    "This notebook inspects the processed corpus to verify text coverage, sidecar availability, and metadata consistency.\n",
    "\n",
    "## Usage\n",
    "\n",
    "Before running this notebook:\n",
    "\n",
    "1. **Set the paper run identifier** (if your data is organized in run-specific subdirectories):\n",
    "   - Option 1: Set environment variable: `export BTC_PAPER_RUN=\"cog_psych_2025_run01\"`\n",
    "   - Option 2: Edit the `PAPER_RUN` variable in the first code cell\n",
    "   - Option 3: Leave as `None` to use the base `data/processed/` and `data/raw/` directories\n",
    "\n",
    "2. **Ensure data is processed**: Run `python scripts/ingest_and_index.py --config configs/default.yaml` first\n",
    "\n",
    "3. **Check paths**: The notebook will print the paths it's using - verify they're correct\n",
    "\n",
    "## What This Notebook Checks\n",
    "\n",
    "- **Manifest validation**: Verifies manifest.json exists and has expected structure\n",
    "- **Text coverage**: Token counts, character counts per document\n",
    "- **Sidecar availability**: Checks for `.pages.jsonl` files\n",
    "- **Math/LaTeX content**: Detects mathematical notation density\n",
    "- **Citation markers**: Finds inline citation patterns like [1], [2]\n",
    "- **Language detection**: Identifies non-English or mixed-language documents\n",
    "- **Metadata consistency**: Validates metadata.jsonl structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a6484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Optional: Load config to use project paths\n",
    "try:\n",
    "    from beyond_the_cutoff.config import load_config\n",
    "\n",
    "    USE_CONFIG = True\n",
    "except ImportError:\n",
    "    USE_CONFIG = False\n",
    "\n",
    "\n",
    "def _find_project_root(start: Path) -> Path:\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / \"pyproject.toml\").exists():\n",
    "            return candidate\n",
    "    return start\n",
    "\n",
    "\n",
    "# Configuration: Set the paper run identifier for this analysis\n",
    "# If your data is organized in run-specific subdirectories, set this.\n",
    "# Otherwise, leave as None or empty string to use the base data directories.\n",
    "PAPER_RUN = os.environ.get(\"BTC_PAPER_RUN\", \"cog_psych_2025_run01\") or None\n",
    "\n",
    "PROJECT_ROOT = _find_project_root(Path.cwd().resolve())\n",
    "\n",
    "# Use config if available, otherwise fall back to hardcoded paths\n",
    "if USE_CONFIG:\n",
    "    try:\n",
    "        config = load_config()\n",
    "        base_processed = config.paths.processed_data\n",
    "        base_raw = config.paths.raw_data\n",
    "    except Exception:\n",
    "        USE_CONFIG = False\n",
    "\n",
    "if not USE_CONFIG:\n",
    "    base_processed = PROJECT_ROOT / \"data/processed\"\n",
    "    base_raw = PROJECT_ROOT / \"data/raw\"\n",
    "\n",
    "# Construct paths with optional run subdirectory\n",
    "if PAPER_RUN:\n",
    "    PROCESSED_DIR = base_processed / PAPER_RUN\n",
    "    RAW_DIR = base_raw / PAPER_RUN\n",
    "else:\n",
    "    PROCESSED_DIR = base_processed\n",
    "    RAW_DIR = base_raw\n",
    "\n",
    "DATA_QUALITY_DIR = PROJECT_ROOT / f\"evaluation/results/data_quality/{PAPER_RUN or 'default'}\"\n",
    "\n",
    "# Manifest is typically at the root of processed_dir, not in a subdirectory\n",
    "MANIFEST_PATH = PROCESSED_DIR / \"manifest.json\"\n",
    "# If manifest is not found, try the parent directory (for flat structure)\n",
    "if not MANIFEST_PATH.exists() and PROCESSED_DIR.parent / \"manifest.json\" != MANIFEST_PATH:\n",
    "    alt_manifest = PROCESSED_DIR.parent / \"manifest.json\"\n",
    "    if alt_manifest.exists():\n",
    "        MANIFEST_PATH = alt_manifest\n",
    "\n",
    "METADATA_JSONL = RAW_DIR / \"metadata.jsonl\"\n",
    "SELECTION_LOG = RAW_DIR / \"selection_log.jsonl\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Processed directory: {PROCESSED_DIR}\")\n",
    "print(f\"Raw directory: {RAW_DIR}\")\n",
    "print(f\"Manifest path: {MANIFEST_PATH}\")\n",
    "print(f\"Manifest exists: {MANIFEST_PATH.exists()}\")\n",
    "\n",
    "plt.rcParams.update({\"figure.figsize\": (8, 4), \"axes.grid\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3a4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def load_manifest(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Processed manifest missing at {path}\\n\"\n",
    "            f\"Run 'python scripts/ingest_and_index.py --config configs/default.yaml' first.\"\n",
    "        )\n",
    "    try:\n",
    "        data = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Manifest file is not valid JSON: {e}\") from e\n",
    "\n",
    "    docs = data.get(\"documents\", [])\n",
    "    if not isinstance(docs, list):\n",
    "        raise TypeError(\"Manifest documents field must be a list\")\n",
    "\n",
    "    records: list[dict[str, Any]] = []\n",
    "    for entry in docs:\n",
    "        if not isinstance(entry, dict):\n",
    "            continue\n",
    "        record = entry.copy()\n",
    "        # Handle both relative and absolute paths in manifest\n",
    "        text_path_str = record.get(\"text_path\", \"\")\n",
    "        if Path(text_path_str).is_absolute():\n",
    "            record[\"text_path\"] = Path(text_path_str)\n",
    "        else:\n",
    "            record[\"text_path\"] = PROCESSED_DIR / text_path_str\n",
    "\n",
    "        pages_path = record.get(\"pages_path\")\n",
    "        if pages_path:\n",
    "            if Path(pages_path).is_absolute():\n",
    "                record[\"pages_path\"] = Path(pages_path)\n",
    "            else:\n",
    "                record[\"pages_path\"] = PROCESSED_DIR / pages_path\n",
    "        else:\n",
    "            record[\"pages_path\"] = None\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    frame = pd.DataFrame(records)\n",
    "    if not frame.empty:\n",
    "        frame[\"has_pages_sidecar\"] = frame[\"pages_path\"].apply(\n",
    "            lambda p: bool(p and Path(p).exists())\n",
    "        )\n",
    "    return frame\n",
    "\n",
    "\n",
    "def load_metadata(jsonl_path: Path) -> pd.DataFrame:\n",
    "    if not jsonl_path.exists():\n",
    "        raise FileNotFoundError(f\"Metadata JSONL missing at {jsonl_path}\")\n",
    "    rows: list[dict[str, Any]] = []\n",
    "    with jsonl_path.open(\"r\", encoding=\"utf-8\") as handle:\n",
    "        for line in handle:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            payload = json.loads(line)\n",
    "            if isinstance(payload, dict):\n",
    "                rows.append(payload)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def estimate_tokens(text_path: Path) -> tuple[int, int]:\n",
    "    text = text_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    words = text.split()\n",
    "    return len(text), len(words)\n",
    "\n",
    "\n",
    "MATH_SYMBOLS = set(\"=±∑∏√∞∂∀∃≤≥×÷∫∇≃≈≠≡⊕⊗⊥⇒⇔→←↔†‡\")\n",
    "LATEX_ENV_PATTERN = re.compile(\n",
    "    r\"\\\\begin{(?:align\\*?|equation\\*?|gather\\*?|multline\\*?|cases|tabular|array)}\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "LATEX_INLINE_PATTERN = re.compile(\n",
    "    r\"\\\\(?:frac|sum|int|sqrt|mathbb|mathrm|mathbf|alpha|beta|gamma|delta|lambda|pi|phi|psi|theta)\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "TABLE_MARKER_PATTERN = re.compile(\n",
    "    r\"\\\\begin{tabular}|\\\\hline|\\\\toprule|\\\\midrule|\\\\bottomrule|&\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "ASCII_TABLE_PATTERN = re.compile(r\"^[+|].*[+|]$\")\n",
    "MATH_TOKEN_PATTERN = re.compile(\n",
    "    r\"(\\\\[A-Za-z]+)|([A-Za-z]*_[A-Za-z0-9]+)|([A-Za-z]*\\\\^[A-Za-z0-9]+)\"\n",
    ")\n",
    "\n",
    "\n",
    "def compute_text_features(text: str) -> dict[str, float]:\n",
    "    tokens = text.split()\n",
    "    lines = text.splitlines()\n",
    "    token_count = len(tokens)\n",
    "    line_count = len(lines)\n",
    "    if token_count == 0:\n",
    "        return {\n",
    "            \"math_token_ratio\": 0.0,\n",
    "            \"inline_math_hits\": 0,\n",
    "            \"latex_env_hits\": 0,\n",
    "            \"table_line_ratio\": 0.0,\n",
    "            \"dollar_inline_hits\": 0,\n",
    "        }\n",
    "\n",
    "    math_tokens = 0\n",
    "    inline_hits = 0\n",
    "    for token in tokens:\n",
    "        if LATEX_INLINE_PATTERN.search(token):\n",
    "            inline_hits += 1\n",
    "        if any(ch in MATH_SYMBOLS for ch in token):\n",
    "            math_tokens += 1\n",
    "            continue\n",
    "        if (\"\\\\\" in token) or (\"^\" in token) or (\"_\" in token):\n",
    "            if MATH_TOKEN_PATTERN.search(token):\n",
    "                math_tokens += 1\n",
    "                continue\n",
    "        digits = sum(ch.isdigit() for ch in token)\n",
    "        if digits >= max(2, len(token) // 2):\n",
    "            math_tokens += 1\n",
    "\n",
    "    latex_env_hits = len(LATEX_ENV_PATTERN.findall(text))\n",
    "    dollar_inline_hits = text.count(\"$\")\n",
    "\n",
    "    table_lines = 0\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if not stripped:\n",
    "            continue\n",
    "        if ASCII_TABLE_PATTERN.match(stripped):\n",
    "            table_lines += 1\n",
    "            continue\n",
    "        if stripped.count(\"|\") >= 3:\n",
    "            table_lines += 1\n",
    "            continue\n",
    "        if TABLE_MARKER_PATTERN.search(stripped):\n",
    "            table_lines += 1\n",
    "\n",
    "    return {\n",
    "        \"math_token_ratio\": math_tokens / token_count,\n",
    "        \"inline_math_hits\": inline_hits,\n",
    "        \"latex_env_hits\": latex_env_hits,\n",
    "        \"table_line_ratio\": table_lines / max(1, line_count),\n",
    "        \"dollar_inline_hits\": dollar_inline_hits,\n",
    "    }\n",
    "\n",
    "\n",
    "def ensure_output_dir(path: Path) -> None:\n",
    "    path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    manifest_df = load_manifest(MANIFEST_PATH)\n",
    "    display(manifest_df.head())\n",
    "    print(f\"Total documents in manifest: {len(manifest_df)}\")\n",
    "    if not manifest_df.empty:\n",
    "        total_bytes = manifest_df[\"bytes\"].sum()\n",
    "        print(f\"Total text bytes: {total_bytes:,}\")\n",
    "        manifest_meta = json.loads(MANIFEST_PATH.read_text(encoding=\"utf-8\"))\n",
    "        print(f\"Manifest generated_at: {manifest_meta.get('generated_at', 'unknown')}\")\n",
    "        print(f\"Manifest total_documents: {manifest_meta.get('total_documents', 'unknown')}\")\n",
    "    else:\n",
    "        print(\"Warning: Manifest is empty!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nTo fix this:\")\n",
    "    print(\"1. Ensure you've run: python scripts/ingest_and_index.py --config configs/default.yaml\")\n",
    "    print(\"2. Check that PROCESSED_DIR is correct (printed above)\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error loading manifest: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7447f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata if available (optional - may not exist for all datasets)\n",
    "if METADATA_JSONL.exists():\n",
    "    try:\n",
    "        metadata_df = load_metadata(METADATA_JSONL)\n",
    "        print(f\"Metadata rows: {len(metadata_df)}\")\n",
    "        if not metadata_df.empty and \"arxiv_id\" in metadata_df.columns:\n",
    "            duplicates = metadata_df[\"arxiv_id\"].str.split(\"v\").str[0].value_counts()\n",
    "            print(f\"Unique arXiv IDs (without version): {len(duplicates)}\")\n",
    "            if len(duplicates) < len(metadata_df):\n",
    "                print(f\"Note: {len(metadata_df) - len(duplicates)} duplicate IDs found\")\n",
    "        display(metadata_df.head() if not metadata_df.empty else None)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to load metadata: {e}\")\n",
    "        metadata_df = pd.DataFrame()\n",
    "else:\n",
    "    print(f\"Metadata file not found at {METADATA_JSONL}\")\n",
    "    print(\"This is optional - metadata.jsonl may not exist for all datasets\")\n",
    "    metadata_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7951e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure manifest_df is loaded before proceeding\n",
    "if \"manifest_df\" not in locals() or manifest_df.empty:\n",
    "    raise RuntimeError(\n",
    "        \"Manifest is empty or not loaded. \" \"Run the previous cell to load the manifest first.\"\n",
    "    )\n",
    "\n",
    "byte_lengths: list[int] = []\n",
    "token_counts: list[int] = []\n",
    "sidecar_pages: list[float] = []\n",
    "documents: list[str] = []\n",
    "math_token_ratios: list[float] = []\n",
    "inline_math_densities: list[float] = []\n",
    "latex_env_densities: list[float] = []\n",
    "table_line_ratios: list[float] = []\n",
    "dollar_inline_densities: list[float] = []\n",
    "citation_marker_totals: list[int] = []\n",
    "citation_marker_unique: list[int] = []\n",
    "max_citation_indices: list[int] = []\n",
    "footnote_url_counts: list[int] = []\n",
    "\n",
    "CITATION_PATTERN = re.compile(r\"\\[(\\d{1,3})\\]\")\n",
    "FOOTNOTE_URL_PATTERN = re.compile(r\"\\n\\d+\\s+https?://\")\n",
    "\n",
    "for row in manifest_df.itertuples(index=False):\n",
    "    text_path = Path(row.text_path)\n",
    "    if not text_path.exists():\n",
    "        continue\n",
    "    documents.append(text_path.name)\n",
    "\n",
    "    chars, tokens = estimate_tokens(text_path)\n",
    "    text = text_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    features = compute_text_features(text)\n",
    "\n",
    "    byte_lengths.append(chars)\n",
    "    token_counts.append(tokens)\n",
    "\n",
    "    tokens_norm = max(1, tokens)\n",
    "    math_token_ratios.append(features[\"math_token_ratio\"])\n",
    "    inline_math_densities.append(features[\"inline_math_hits\"] / tokens_norm * 1000)\n",
    "    latex_env_densities.append(features[\"latex_env_hits\"] / tokens_norm * 1000)\n",
    "    table_line_ratios.append(features[\"table_line_ratio\"])\n",
    "    dollar_inline_densities.append(features[\"dollar_inline_hits\"] / tokens_norm * 1000)\n",
    "\n",
    "    citations = [int(mark) for mark in CITATION_PATTERN.findall(text)]\n",
    "    citation_marker_totals.append(len(citations))\n",
    "    if citations:\n",
    "        unique_citations = set(citations)\n",
    "        citation_marker_unique.append(len(unique_citations))\n",
    "        max_citation_indices.append(max(unique_citations))\n",
    "    else:\n",
    "        citation_marker_unique.append(0)\n",
    "        max_citation_indices.append(0)\n",
    "\n",
    "    footnote_url_counts.append(len(FOOTNOTE_URL_PATTERN.findall(text)))\n",
    "\n",
    "    pages_path = row.pages_path\n",
    "    if pages_path:\n",
    "        with Path(pages_path).open(\"r\", encoding=\"utf-8\") as handle:\n",
    "            page_counter = sum(1 for _ in handle)\n",
    "    else:\n",
    "        page_counter = float(\"nan\")\n",
    "    sidecar_pages.append(page_counter)\n",
    "\n",
    "summary = pd.DataFrame(\n",
    "    {\n",
    "        \"document\": documents,\n",
    "        \"tokens\": token_counts,\n",
    "        \"chars\": byte_lengths,\n",
    "        \"sidecar_pages\": sidecar_pages,\n",
    "        \"math_token_ratio\": math_token_ratios,\n",
    "        \"inline_math_per_1k_tokens\": inline_math_densities,\n",
    "        \"latex_env_per_1k_tokens\": latex_env_densities,\n",
    "        \"table_line_ratio\": table_line_ratios,\n",
    "        \"inline_dollar_per_1k_tokens\": dollar_inline_densities,\n",
    "        \"citation_marker_total\": citation_marker_totals,\n",
    "        \"citation_marker_unique\": citation_marker_unique,\n",
    "        \"max_citation_index\": max_citation_indices,\n",
    "        \"footnote_url_count\": footnote_url_counts,\n",
    "    }\n",
    ")\n",
    "numeric_cols = summary.select_dtypes(include=\"number\").columns\n",
    "summary_stats = summary[numeric_cols].describe(percentiles=[0.05, 0.5, 0.95])\n",
    "display(summary_stats)\n",
    "\n",
    "print(\"Documents missing sidecars:\", (~manifest_df[\"has_pages_sidecar\"]).sum())\n",
    "print(\n",
    "    \"Documents containing [n] citation markers:\",\n",
    "    (summary[\"citation_marker_total\"] > 0).sum(),\n",
    "    f\"/ {len(summary)}\",\n",
    ")\n",
    "print(\n",
    "    \"Max citation index observed:\",\n",
    "    int(summary[\"max_citation_index\"].max()) if not summary.empty else 0,\n",
    ")\n",
    "print(\n",
    "    \"Documents with footnote-style URL markers:\",\n",
    "    (summary[\"footnote_url_count\"] > 0).sum(),\n",
    ")\n",
    "\n",
    "if not summary.empty:\n",
    "    top_token_df = summary.sort_values(\"tokens\", ascending=False).head(5)[\n",
    "        [\n",
    "            \"document\",\n",
    "            \"tokens\",\n",
    "            \"citation_marker_total\",\n",
    "            \"citation_marker_unique\",\n",
    "            \"footnote_url_count\",\n",
    "        ]\n",
    "    ]\n",
    "    print(\"Top 5 documents by token count:\")\n",
    "    display(top_token_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92143f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].hist(summary[\"tokens\"], bins=30, color=\"#1f77b4\")\n",
    "axes[0].set_title(\"Token count distribution\")\n",
    "axes[0].set_xlabel(\"Tokens per document\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "axes[1].hist(summary[\"sidecar_pages\"].dropna(), bins=30, color=\"#ff7f0e\")\n",
    "axes[1].set_title(\"Pages per sidecar\")\n",
    "axes[1].set_xlabel(\"Pages\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bd97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not summary.empty:\n",
    "    print(\"Most math-heavy documents (top 5):\")\n",
    "    display(\n",
    "        summary.sort_values(\"math_token_ratio\", ascending=False).head(5)[\n",
    "            [\"document\", \"math_token_ratio\", \"inline_math_per_1k_tokens\", \"latex_env_per_1k_tokens\"]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"Documents with table-like structure signals (top 5):\")\n",
    "    display(\n",
    "        summary.sort_values(\"table_line_ratio\", ascending=False).head(5)[\n",
    "            [\"document\", \"table_line_ratio\", \"inline_dollar_per_1k_tokens\"]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    high_math = summary[summary[\"math_token_ratio\"] > 0.25]\n",
    "    if not high_math.empty:\n",
    "        print(f\"Documents exceeding math token ratio 0.25: {len(high_math)}\")\n",
    "\n",
    "    table_heavy = summary[summary[\"table_line_ratio\"] > 0.15]\n",
    "    if not table_heavy.empty:\n",
    "        print(f\"Documents exceeding table-line ratio 0.15: {len(table_heavy)}\")\n",
    "else:\n",
    "    print(\"No documents available for math/table analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee4fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language detection requires langid package\n",
    "# Install with: pip install langid\n",
    "try:\n",
    "    import langid\n",
    "\n",
    "    LANGID_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LANGID_AVAILABLE = False\n",
    "    print(\"Warning: langid package not available. Language detection will be skipped.\")\n",
    "    print(\"Install with: pip install langid\")\n",
    "\n",
    "if LANGID_AVAILABLE:\n",
    "    # Detect languages for each document using multiple samples\n",
    "    detected_languages = []\n",
    "    non_english_docs = []\n",
    "    confidences = []\n",
    "    doc_details = []\n",
    "    mixed_language_docs = []\n",
    "\n",
    "    def get_samples(text: str, num_samples: int = 3, sample_size: int = 1000):\n",
    "        \"\"\"Get multiple samples from the text.\"\"\"\n",
    "        length = len(text)\n",
    "        if length <= sample_size:\n",
    "            return [text]\n",
    "        samples = []\n",
    "        step = length // (num_samples + 1)\n",
    "        for i in range(1, num_samples + 1):\n",
    "            start = i * step\n",
    "            end = min(start + sample_size, length)\n",
    "            samples.append(text[start:end])\n",
    "        return samples\n",
    "\n",
    "    for row in manifest_df.itertuples(index=False):\n",
    "        text_path = Path(row.text_path)\n",
    "        if not text_path.exists():\n",
    "            continue\n",
    "        try:\n",
    "            text = text_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "            samples = get_samples(text, num_samples=3, sample_size=2000)\n",
    "            langs = []\n",
    "            confs = []\n",
    "            for sample in samples:\n",
    "                if sample.strip():\n",
    "                    lang, conf = langid.classify(sample)\n",
    "                    langs.append(lang)\n",
    "                    confs.append(conf)\n",
    "            # Use the most common language, or if tie, the one with highest confidence\n",
    "            if langs:\n",
    "                lang_counts = Counter(langs)\n",
    "                most_common = lang_counts.most_common(1)[0][0]\n",
    "                # Average confidence for that language\n",
    "                avg_conf = sum(\n",
    "                    conf\n",
    "                    for lang_code, conf in zip(langs, confs, strict=False)\n",
    "                    if lang_code == most_common\n",
    "                ) / langs.count(most_common)\n",
    "                detected_languages.append(most_common)\n",
    "                confidences.append(avg_conf)\n",
    "                doc_details.append((text_path.name, most_common, avg_conf, langs, confs))\n",
    "                if most_common != \"en\":\n",
    "                    non_english_docs.append((text_path.name, most_common, avg_conf))\n",
    "                # Check if any sample is not English\n",
    "                if any(lang_code != \"en\" for lang_code in langs):\n",
    "                    mixed_language_docs.append((text_path.name, langs, confs))\n",
    "            else:\n",
    "                detected_languages.append(\"unknown\")\n",
    "                confidences.append(0.0)\n",
    "                doc_details.append((text_path.name, \"unknown\", 0.0, [], []))\n",
    "                non_english_docs.append((text_path.name, \"unknown\", 0.0))\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to process {text_path.name}: {e}\")\n",
    "            detected_languages.append(\"unknown\")\n",
    "            confidences.append(0.0)\n",
    "            doc_details.append((text_path.name, \"unknown\", 0.0, [], []))\n",
    "            non_english_docs.append((text_path.name, \"unknown\", 0.0))\n",
    "\n",
    "    # Summary of languages\n",
    "    lang_counts = Counter(detected_languages)\n",
    "    print(\"Language distribution:\")\n",
    "    for lang, count in lang_counts.most_common():\n",
    "        print(f\"{lang}: {count}\")\n",
    "\n",
    "    print(f\"\\nTotal documents: {len(detected_languages)}\")\n",
    "    print(f\"Non-English documents: {len(non_english_docs)}\")\n",
    "    print(f\"Documents with mixed/foreign language content: {len(mixed_language_docs)}\")\n",
    "\n",
    "    if non_english_docs:\n",
    "        print(\"\\nNon-English documents:\")\n",
    "        for doc, lang, conf in non_english_docs[:10]:  # Show first 10\n",
    "            print(f\"{doc}: {lang} (avg confidence: {conf:.2f})\")\n",
    "        if len(non_english_docs) > 10:\n",
    "            print(f\"... and {len(non_english_docs) - 10} more\")\n",
    "    else:\n",
    "        print(\"All documents appear to be in English.\")\n",
    "\n",
    "    if mixed_language_docs:\n",
    "        print(\"\\nDocuments with mixed/foreign language content:\")\n",
    "        for doc, langs, confs in mixed_language_docs:\n",
    "            print(f\"{doc}: languages {langs}, confidences {[f'{c:.2f}' for c in confs]}\")\n",
    "    else:\n",
    "        print(\"No documents with mixed language content detected.\")\n",
    "\n",
    "    # Show average confidence\n",
    "    if confidences:\n",
    "        avg_conf = sum(confidences) / len(confidences)\n",
    "        print(f\"\\nAverage language detection confidence: {avg_conf:.2f}\")\n",
    "else:\n",
    "    print(\"Skipping language detection (langid not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e1732",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_output_dir(DATA_QUALITY_DIR / \"plots\")\n",
    "output_csv = DATA_QUALITY_DIR / \"qa_summary_stats.csv\"\n",
    "summary_stats.to_csv(output_csv, index=True)\n",
    "print(f\"Wrote summary stats to {output_csv}\")\n",
    "\n",
    "selection_entries: list[dict[str, Any]] = []\n",
    "if SELECTION_LOG.exists():\n",
    "    with SELECTION_LOG.open(\"r\", encoding=\"utf-8\") as handle:\n",
    "        for line in handle:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            selection_entries.append(json.loads(line))\n",
    "if selection_entries:\n",
    "    batches = Counter(entry.get(\"selection_batch\", \"unknown\") for entry in selection_entries)\n",
    "    print(\"Selection batches recorded:\", batches)\n",
    "else:\n",
    "    print(\"No selection log entries found; ensure selection_log.jsonl is populated in future runs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
