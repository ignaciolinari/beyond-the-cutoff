{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9dadf17",
   "metadata": {},
   "source": [
    "# Corpus Quality Checks\n",
    "\n",
    "This notebook inspects the processed corpus to verify text coverage, sidecar availability, and metadata consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a6484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _find_project_root(start: Path) -> Path:\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / \"pyproject.toml\").exists():\n",
    "            return candidate\n",
    "    return start\n",
    "\n",
    "\n",
    "# Set the paper run identifier for this analysis\n",
    "PAPER_RUN = \"cog_psych_2025_run01\"\n",
    "PROJECT_ROOT = _find_project_root(Path.cwd().resolve())\n",
    "PROCESSED_DIR = PROJECT_ROOT / f\"data/processed/{PAPER_RUN}\"\n",
    "RAW_DIR = PROJECT_ROOT / f\"data/raw/{PAPER_RUN}\"\n",
    "DATA_QUALITY_DIR = PROJECT_ROOT / f\"evaluation/results/data_quality/{PAPER_RUN}\"\n",
    "\n",
    "MANIFEST_PATH = PROCESSED_DIR / \"manifest.json\"\n",
    "METADATA_JSONL = RAW_DIR / \"metadata.jsonl\"\n",
    "SELECTION_LOG = RAW_DIR / \"selection_log.jsonl\"\n",
    "\n",
    "plt.rcParams.update({\"figure.figsize\": (8, 4), \"axes.grid\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3a4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def load_manifest(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Processed manifest missing at {path}\")\n",
    "    data = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    docs = data.get(\"documents\", [])\n",
    "    if not isinstance(docs, list):\n",
    "        raise TypeError(\"Manifest documents field must be a list\")\n",
    "    records: list[dict[str, Any]] = []\n",
    "    for entry in docs:\n",
    "        if not isinstance(entry, dict):\n",
    "            continue\n",
    "        record = entry.copy()\n",
    "        record[\"text_path\"] = PROCESSED_DIR / record[\"text_path\"]\n",
    "        pages_path = record.get(\"pages_path\")\n",
    "        record[\"pages_path\"] = PROCESSED_DIR / pages_path if pages_path else None\n",
    "        records.append(record)\n",
    "    frame = pd.DataFrame(records)\n",
    "    if not frame.empty:\n",
    "        frame[\"has_pages_sidecar\"] = frame[\"pages_path\"].apply(\n",
    "            lambda p: bool(p and Path(p).exists())\n",
    "        )\n",
    "    return frame\n",
    "\n",
    "\n",
    "def load_metadata(jsonl_path: Path) -> pd.DataFrame:\n",
    "    if not jsonl_path.exists():\n",
    "        raise FileNotFoundError(f\"Metadata JSONL missing at {jsonl_path}\")\n",
    "    rows: list[dict[str, Any]] = []\n",
    "    with jsonl_path.open(\"r\", encoding=\"utf-8\") as handle:\n",
    "        for line in handle:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            payload = json.loads(line)\n",
    "            if isinstance(payload, dict):\n",
    "                rows.append(payload)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def estimate_tokens(text_path: Path) -> tuple[int, int]:\n",
    "    text = text_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    words = text.split()\n",
    "    return len(text), len(words)\n",
    "\n",
    "\n",
    "MATH_SYMBOLS = set(\"=±∑∏√∞∂∀∃≤≥×÷∫∇≃≈≠≡⊕⊗⊥⇒⇔→←↔†‡\")\n",
    "LATEX_ENV_PATTERN = re.compile(\n",
    "    r\"\\\\begin{(?:align\\*?|equation\\*?|gather\\*?|multline\\*?|cases|tabular|array)}\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "LATEX_INLINE_PATTERN = re.compile(\n",
    "    r\"\\\\(?:frac|sum|int|sqrt|mathbb|mathrm|mathbf|alpha|beta|gamma|delta|lambda|pi|phi|psi|theta)\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "TABLE_MARKER_PATTERN = re.compile(\n",
    "    r\"\\\\begin{tabular}|\\\\hline|\\\\toprule|\\\\midrule|\\\\bottomrule|&\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "ASCII_TABLE_PATTERN = re.compile(r\"^[+|].*[+|]$\")\n",
    "MATH_TOKEN_PATTERN = re.compile(\n",
    "    r\"(\\\\[A-Za-z]+)|([A-Za-z]*_[A-Za-z0-9]+)|([A-Za-z]*\\\\^[A-Za-z0-9]+)\"\n",
    ")\n",
    "\n",
    "\n",
    "def compute_text_features(text: str) -> dict[str, float]:\n",
    "    tokens = text.split()\n",
    "    lines = text.splitlines()\n",
    "    token_count = len(tokens)\n",
    "    line_count = len(lines)\n",
    "    if token_count == 0:\n",
    "        return {\n",
    "            \"math_token_ratio\": 0.0,\n",
    "            \"inline_math_hits\": 0,\n",
    "            \"latex_env_hits\": 0,\n",
    "            \"table_line_ratio\": 0.0,\n",
    "            \"dollar_inline_hits\": 0,\n",
    "        }\n",
    "\n",
    "    math_tokens = 0\n",
    "    inline_hits = 0\n",
    "    for token in tokens:\n",
    "        if LATEX_INLINE_PATTERN.search(token):\n",
    "            inline_hits += 1\n",
    "        if any(ch in MATH_SYMBOLS for ch in token):\n",
    "            math_tokens += 1\n",
    "            continue\n",
    "        if (\"\\\\\" in token) or (\"^\" in token) or (\"_\" in token):\n",
    "            if MATH_TOKEN_PATTERN.search(token):\n",
    "                math_tokens += 1\n",
    "                continue\n",
    "        digits = sum(ch.isdigit() for ch in token)\n",
    "        if digits >= max(2, len(token) // 2):\n",
    "            math_tokens += 1\n",
    "\n",
    "    latex_env_hits = len(LATEX_ENV_PATTERN.findall(text))\n",
    "    dollar_inline_hits = text.count(\"$\")\n",
    "\n",
    "    table_lines = 0\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if not stripped:\n",
    "            continue\n",
    "        if ASCII_TABLE_PATTERN.match(stripped):\n",
    "            table_lines += 1\n",
    "            continue\n",
    "        if stripped.count(\"|\") >= 3:\n",
    "            table_lines += 1\n",
    "            continue\n",
    "        if TABLE_MARKER_PATTERN.search(stripped):\n",
    "            table_lines += 1\n",
    "\n",
    "    return {\n",
    "        \"math_token_ratio\": math_tokens / token_count,\n",
    "        \"inline_math_hits\": inline_hits,\n",
    "        \"latex_env_hits\": latex_env_hits,\n",
    "        \"table_line_ratio\": table_lines / max(1, line_count),\n",
    "        \"dollar_inline_hits\": dollar_inline_hits,\n",
    "    }\n",
    "\n",
    "\n",
    "def ensure_output_dir(path: Path) -> None:\n",
    "    path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_df = load_manifest(MANIFEST_PATH)\n",
    "display(manifest_df.head())\n",
    "print(f\"Total documents in manifest: {len(manifest_df)}\")\n",
    "if not manifest_df.empty:\n",
    "    total_bytes = manifest_df[\"bytes\"].sum()\n",
    "    print(f\"Total text bytes: {total_bytes:,}\")\n",
    "    manifest_meta = json.loads(MANIFEST_PATH.read_text(encoding=\"utf-8\"))\n",
    "    print(f\"Manifest generated_at: {manifest_meta.get('generated_at', 'unknown')}\")\n",
    "    print(f\"Manifest total_documents: {manifest_meta.get('total_documents', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7447f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = load_metadata(METADATA_JSONL)\n",
    "print(f\"Metadata rows: {len(metadata_df)}\")\n",
    "if not metadata_df.empty:\n",
    "    duplicates = metadata_df[\"arxiv_id\"].str.split(\"v\").str[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7951e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if manifest_df.empty:\n",
    "    raise RuntimeError(\"Manifest is empty; aborting QA metrics.\")\n",
    "\n",
    "byte_lengths: list[int] = []\n",
    "token_counts: list[int] = []\n",
    "sidecar_pages: list[float] = []\n",
    "documents: list[str] = []\n",
    "math_token_ratios: list[float] = []\n",
    "inline_math_densities: list[float] = []\n",
    "latex_env_densities: list[float] = []\n",
    "table_line_ratios: list[float] = []\n",
    "dollar_inline_densities: list[float] = []\n",
    "citation_marker_totals: list[int] = []\n",
    "citation_marker_unique: list[int] = []\n",
    "max_citation_indices: list[int] = []\n",
    "footnote_url_counts: list[int] = []\n",
    "\n",
    "CITATION_PATTERN = re.compile(r\"\\[(\\d{1,3})\\]\")\n",
    "FOOTNOTE_URL_PATTERN = re.compile(r\"\\n\\d+\\s+https?://\")\n",
    "\n",
    "for row in manifest_df.itertuples(index=False):\n",
    "    text_path = Path(row.text_path)\n",
    "    if not text_path.exists():\n",
    "        continue\n",
    "    documents.append(text_path.name)\n",
    "\n",
    "    chars, tokens = estimate_tokens(text_path)\n",
    "    text = text_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    features = compute_text_features(text)\n",
    "\n",
    "    byte_lengths.append(chars)\n",
    "    token_counts.append(tokens)\n",
    "\n",
    "    tokens_norm = max(1, tokens)\n",
    "    math_token_ratios.append(features[\"math_token_ratio\"])\n",
    "    inline_math_densities.append(features[\"inline_math_hits\"] / tokens_norm * 1000)\n",
    "    latex_env_densities.append(features[\"latex_env_hits\"] / tokens_norm * 1000)\n",
    "    table_line_ratios.append(features[\"table_line_ratio\"])\n",
    "    dollar_inline_densities.append(features[\"dollar_inline_hits\"] / tokens_norm * 1000)\n",
    "\n",
    "    citations = [int(mark) for mark in CITATION_PATTERN.findall(text)]\n",
    "    citation_marker_totals.append(len(citations))\n",
    "    if citations:\n",
    "        unique_citations = set(citations)\n",
    "        citation_marker_unique.append(len(unique_citations))\n",
    "        max_citation_indices.append(max(unique_citations))\n",
    "    else:\n",
    "        citation_marker_unique.append(0)\n",
    "        max_citation_indices.append(0)\n",
    "\n",
    "    footnote_url_counts.append(len(FOOTNOTE_URL_PATTERN.findall(text)))\n",
    "\n",
    "    pages_path = row.pages_path\n",
    "    if pages_path:\n",
    "        with Path(pages_path).open(\"r\", encoding=\"utf-8\") as handle:\n",
    "            page_counter = sum(1 for _ in handle)\n",
    "    else:\n",
    "        page_counter = float(\"nan\")\n",
    "    sidecar_pages.append(page_counter)\n",
    "\n",
    "summary = pd.DataFrame(\n",
    "    {\n",
    "        \"document\": documents,\n",
    "        \"tokens\": token_counts,\n",
    "        \"chars\": byte_lengths,\n",
    "        \"sidecar_pages\": sidecar_pages,\n",
    "        \"math_token_ratio\": math_token_ratios,\n",
    "        \"inline_math_per_1k_tokens\": inline_math_densities,\n",
    "        \"latex_env_per_1k_tokens\": latex_env_densities,\n",
    "        \"table_line_ratio\": table_line_ratios,\n",
    "        \"inline_dollar_per_1k_tokens\": dollar_inline_densities,\n",
    "        \"citation_marker_total\": citation_marker_totals,\n",
    "        \"citation_marker_unique\": citation_marker_unique,\n",
    "        \"max_citation_index\": max_citation_indices,\n",
    "        \"footnote_url_count\": footnote_url_counts,\n",
    "    }\n",
    ")\n",
    "numeric_cols = summary.select_dtypes(include=\"number\").columns\n",
    "summary_stats = summary[numeric_cols].describe(percentiles=[0.05, 0.5, 0.95])\n",
    "display(summary_stats)\n",
    "\n",
    "print(\"Documents missing sidecars:\", (~manifest_df[\"has_pages_sidecar\"]).sum())\n",
    "print(\n",
    "    \"Documents containing [n] citation markers:\",\n",
    "    (summary[\"citation_marker_total\"] > 0).sum(),\n",
    "    f\"/ {len(summary)}\",\n",
    ")\n",
    "print(\n",
    "    \"Max citation index observed:\",\n",
    "    int(summary[\"max_citation_index\"].max()) if not summary.empty else 0,\n",
    ")\n",
    "print(\n",
    "    \"Documents with footnote-style URL markers:\",\n",
    "    (summary[\"footnote_url_count\"] > 0).sum(),\n",
    ")\n",
    "\n",
    "if not summary.empty:\n",
    "    top_token_df = summary.sort_values(\"tokens\", ascending=False).head(5)[\n",
    "        [\n",
    "            \"document\",\n",
    "            \"tokens\",\n",
    "            \"citation_marker_total\",\n",
    "            \"citation_marker_unique\",\n",
    "            \"footnote_url_count\",\n",
    "        ]\n",
    "    ]\n",
    "    print(\"Top 5 documents by token count:\")\n",
    "    display(top_token_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92143f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].hist(summary[\"tokens\"], bins=30, color=\"#1f77b4\")\n",
    "axes[0].set_title(\"Token count distribution\")\n",
    "axes[0].set_xlabel(\"Tokens per document\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "axes[1].hist(summary[\"sidecar_pages\"].dropna(), bins=30, color=\"#ff7f0e\")\n",
    "axes[1].set_title(\"Pages per sidecar\")\n",
    "axes[1].set_xlabel(\"Pages\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bd97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not summary.empty:\n",
    "    print(\"Most math-heavy documents (top 5):\")\n",
    "    display(\n",
    "        summary.sort_values(\"math_token_ratio\", ascending=False).head(5)[\n",
    "            [\"document\", \"math_token_ratio\", \"inline_math_per_1k_tokens\", \"latex_env_per_1k_tokens\"]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"Documents with table-like structure signals (top 5):\")\n",
    "    display(\n",
    "        summary.sort_values(\"table_line_ratio\", ascending=False).head(5)[\n",
    "            [\"document\", \"table_line_ratio\", \"inline_dollar_per_1k_tokens\"]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    high_math = summary[summary[\"math_token_ratio\"] > 0.25]\n",
    "    if not high_math.empty:\n",
    "        print(f\"Documents exceeding math token ratio 0.25: {len(high_math)}\")\n",
    "\n",
    "    table_heavy = summary[summary[\"table_line_ratio\"] > 0.15]\n",
    "    if not table_heavy.empty:\n",
    "        print(f\"Documents exceeding table-line ratio 0.15: {len(table_heavy)}\")\n",
    "else:\n",
    "    print(\"No documents available for math/table analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee4fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langid\n",
    "\n",
    "# Detect languages for each document using multiple samples\n",
    "detected_languages = []\n",
    "non_english_docs = []\n",
    "confidences = []\n",
    "doc_details = []\n",
    "mixed_language_docs = []\n",
    "\n",
    "\n",
    "def get_samples(text: str, num_samples: int = 3, sample_size: int = 1000):\n",
    "    \"\"\"Get multiple samples from the text.\"\"\"\n",
    "    length = len(text)\n",
    "    if length <= sample_size:\n",
    "        return [text]\n",
    "    samples = []\n",
    "    step = length // (num_samples + 1)\n",
    "    for i in range(1, num_samples + 1):\n",
    "        start = i * step\n",
    "        end = min(start + sample_size, length)\n",
    "        samples.append(text[start:end])\n",
    "    return samples\n",
    "\n",
    "\n",
    "for row in manifest_df.itertuples(index=False):\n",
    "    text_path = Path(row.text_path)\n",
    "    if not text_path.exists():\n",
    "        continue\n",
    "    try:\n",
    "        text = text_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        samples = get_samples(text, num_samples=3, sample_size=2000)\n",
    "        langs = []\n",
    "        confs = []\n",
    "        for sample in samples:\n",
    "            if sample.strip():\n",
    "                lang, conf = langid.classify(sample)\n",
    "                langs.append(lang)\n",
    "                confs.append(conf)\n",
    "        # Use the most common language, or if tie, the one with highest confidence\n",
    "        if langs:\n",
    "            lang_counts = Counter(langs)\n",
    "            most_common = lang_counts.most_common(1)[0][0]\n",
    "            # Average confidence for that language\n",
    "            avg_conf = sum(\n",
    "                conf\n",
    "                for lang_code, conf in zip(langs, confs, strict=False)\n",
    "                if lang_code == most_common\n",
    "            ) / langs.count(most_common)\n",
    "            detected_languages.append(most_common)\n",
    "            confidences.append(avg_conf)\n",
    "            doc_details.append((text_path.name, most_common, avg_conf, langs, confs))\n",
    "            if most_common != \"en\":\n",
    "                non_english_docs.append((text_path.name, most_common, avg_conf))\n",
    "            # Check if any sample is not English\n",
    "            if any(lang_code != \"en\" for lang_code in langs):\n",
    "                mixed_language_docs.append((text_path.name, langs, confs))\n",
    "        else:\n",
    "            detected_languages.append(\"unknown\")\n",
    "            confidences.append(0.0)\n",
    "            doc_details.append((text_path.name, \"unknown\", 0.0, [], []))\n",
    "            non_english_docs.append((text_path.name, \"unknown\", 0.0))\n",
    "    except Exception:\n",
    "        detected_languages.append(\"unknown\")\n",
    "        confidences.append(0.0)\n",
    "        doc_details.append((text_path.name, \"unknown\", 0.0, [], []))\n",
    "        non_english_docs.append((text_path.name, \"unknown\", 0.0))\n",
    "\n",
    "# Summary of languages\n",
    "lang_counts = Counter(detected_languages)\n",
    "print(\"Language distribution:\")\n",
    "for lang, count in lang_counts.most_common():\n",
    "    print(f\"{lang}: {count}\")\n",
    "\n",
    "print(f\"\\nTotal documents: {len(detected_languages)}\")\n",
    "print(f\"Non-English documents: {len(non_english_docs)}\")\n",
    "print(f\"Documents with mixed/foreign language content: {len(mixed_language_docs)}\")\n",
    "\n",
    "if non_english_docs:\n",
    "    print(\"\\nNon-English documents:\")\n",
    "    for doc, lang, conf in non_english_docs[:10]:  # Show first 10\n",
    "        print(f\"{doc}: {lang} (avg confidence: {conf:.2f})\")\n",
    "    if len(non_english_docs) > 10:\n",
    "        print(f\"... and {len(non_english_docs) - 10} more\")\n",
    "else:\n",
    "    print(\"All documents appear to be in English.\")\n",
    "\n",
    "if mixed_language_docs:\n",
    "    print(\"\\nDocuments with mixed/foreign language content:\")\n",
    "    for doc, langs, confs in mixed_language_docs:\n",
    "        print(f\"{doc}: languages {langs}, confidences {[f'{c:.2f}' for c in confs]}\")\n",
    "else:\n",
    "    print(\"No documents with mixed language content detected.\")\n",
    "\n",
    "# Show average confidence\n",
    "if confidences:\n",
    "    avg_conf = sum(confidences) / len(confidences)\n",
    "    print(f\"\\nAverage language detection confidence: {avg_conf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e1732",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_output_dir(DATA_QUALITY_DIR / \"plots\")\n",
    "output_csv = DATA_QUALITY_DIR / \"qa_summary_stats.csv\"\n",
    "summary_stats.to_csv(output_csv, index=True)\n",
    "print(f\"Wrote summary stats to {output_csv}\")\n",
    "\n",
    "selection_entries: list[dict[str, Any]] = []\n",
    "if SELECTION_LOG.exists():\n",
    "    with SELECTION_LOG.open(\"r\", encoding=\"utf-8\") as handle:\n",
    "        for line in handle:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            selection_entries.append(json.loads(line))\n",
    "if selection_entries:\n",
    "    batches = Counter(entry.get(\"selection_batch\", \"unknown\") for entry in selection_entries)\n",
    "    print(\"Selection batches recorded:\", batches)\n",
    "else:\n",
    "    print(\"No selection log entries found; ensure selection_log.jsonl is populated in future runs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
