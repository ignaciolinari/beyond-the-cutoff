{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adfd5710",
   "metadata": {},
   "source": [
    "# Judge Evaluation Review Notebook\n",
    "\n",
    "This notebook inspects evaluation results to understand how the judge scored model responses. It analyzes judge scores, citation metrics, and provides visualizations.\n",
    "\n",
    "## Usage\n",
    "\n",
    "**Option 1: Analyze specific results directory**\n",
    "- Set `RESULTS_DIR` in the code cell below (e.g., `\"rag_baseline_0p5b\"`)\n",
    "- The notebook will look for `metrics.json` in `evaluation/results/{RESULTS_DIR}/`\n",
    "\n",
    "**Option 2: Analyze specific metrics file**\n",
    "- Set `METRICS_PATH` directly to point to any `metrics.json` file\n",
    "\n",
    "**Default**: Uses `subset20/rag_baseline.json` (legacy path - update for current experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844a9890",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Load the summary JSON, unpack the per-example records, and derive helper columns that capture citation behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c458326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68a982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_project_root(markers=(\"pyproject.toml\", \".git\")):\n",
    "    \"\"\"Return the repository root by looking for a known anchor file.\"\"\"\n",
    "    current = Path.cwd().resolve()\n",
    "    for candidate in (current, *current.parents):\n",
    "        if any((candidate / marker).exists() for marker in markers):\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\n",
    "        \"Unable to locate project root; run this notebook from inside the repository\"\n",
    "    )\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "\n",
    "# Configuration: Set the results directory or metrics file path\n",
    "# Option 1: Specify a results directory (will look for metrics.json inside)\n",
    "RESULTS_DIR = None  # e.g., \"rag_baseline_0p5b\", \"lora_science_0p5b_ft_only\", \"hybrid_science_0p5b\"\n",
    "\n",
    "# Option 2: Specify full path to metrics.json file directly\n",
    "METRICS_PATH = None  # e.g., Path(\"evaluation/results/rag_baseline_0p5b/metrics.json\")\n",
    "\n",
    "# Legacy: Default to old subset20 path if nothing specified\n",
    "if RESULTS_DIR:\n",
    "    summary_path = PROJECT_ROOT / \"evaluation/results\" / RESULTS_DIR / \"metrics.json\"\n",
    "elif METRICS_PATH:\n",
    "    summary_path = Path(METRICS_PATH) if isinstance(METRICS_PATH, str) else METRICS_PATH\n",
    "else:\n",
    "    # Legacy fallback - update this for your current experiments\n",
    "    RESULTS_ROOT = PROJECT_ROOT / \"evaluation/results/subset20\"\n",
    "    summary_path = RESULTS_ROOT / \"rag_baseline.json\"\n",
    "    print(f\"Warning: Using legacy path {summary_path}\")\n",
    "    print(\"Update RESULTS_DIR or METRICS_PATH in this cell to analyze current experiments\")\n",
    "\n",
    "if not summary_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Metrics file not found at {summary_path}\\n\"\n",
    "        f\"Update RESULTS_DIR or METRICS_PATH in the cell above to point to your evaluation results.\"\n",
    "    )\n",
    "\n",
    "print(f\"Analyzing results from: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae424580",
   "metadata": {},
   "outputs": [],
   "source": [
    "with summary_path.open(encoding=\"utf-8\") as fh:\n",
    "    payload = json.load(fh)\n",
    "\n",
    "summary = payload[\"summary\"]\n",
    "examples = payload[\"examples\"]\n",
    "df = pd.DataFrame(examples)\n",
    "df[\"mean_coverage\"] = df[\"citation_metrics\"].apply(\n",
    "    lambda row: row.get(\"mean_coverage\", float(\"nan\"))\n",
    ")\n",
    "df[\"missing_citations\"] = df[\"citation_metrics\"].apply(lambda row: len(row.get(\"missing\", [])))\n",
    "df[\"extra_citations\"] = df[\"citation_metrics\"].apply(lambda row: len(row.get(\"extra\", [])))\n",
    "df[\"referenced_citations\"] = df[\"citation_metrics\"].apply(\n",
    "    lambda row: len(row.get(\"referenced\", []))\n",
    ")\n",
    "\n",
    "\n",
    "def extract_score(scores, target):\n",
    "    if isinstance(scores, dict):\n",
    "        value = scores.get(target)\n",
    "        if value is not None:\n",
    "            return float(value)\n",
    "    return float(\"nan\")\n",
    "\n",
    "\n",
    "score_keys = [\"factuality\", \"grounding\", \"completeness\", \"communication\"]\n",
    "for key in score_keys:\n",
    "    column_name = f\"score_{key}\"\n",
    "    df[column_name] = df[\"judge_scores\"].apply(extract_score, target=key)\n",
    "df[\"verdict\"] = df[\"judge_verdict\"].fillna(\"\").str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e5edb",
   "metadata": {},
   "source": [
    "## Run Summary\n",
    "The JSON summary provides the top-level evaluation context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc85141",
   "metadata": {},
   "source": [
    "## Judge Score Summary\n",
    "Mean and distribution of the structured judge rubric scores and pass rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dcbfd0",
   "metadata": {},
   "source": [
    "**Dimension Glossary**\n",
    "- **Factuality**: penalises hallucinated or contradictory claims; rewards scientifically correct statements.\n",
    "- **Grounding**: checks that answers rely on the provided sources rather than speculation.\n",
    "- **Completeness**: captures how fully the response covers each question requirement.\n",
    "- **Communication**: reflects clarity, structure, and adherence to the requested style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24714637",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_columns = [col for col in df.columns if col.startswith(\"score_\")]\n",
    "score_overview = (\n",
    "    df[score_columns]\n",
    "    .agg([\"mean\", \"median\", \"std\", \"min\", \"max\"])\n",
    "    .T.rename_axis(\"dimension\")\n",
    "    .reset_index()\n",
    ")\n",
    "score_overview[\"dimension\"] = score_overview[\"dimension\"].str.replace(\"score_\", \"\").str.title()\n",
    "display(score_overview.round(3))\n",
    "\n",
    "pass_rate = df[\"verdict\"].eq(\"pass\").mean()\n",
    "print(f\"Pass rate: {pass_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e4092",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "df[score_columns].boxplot(ax=ax)\n",
    "ax.set_title(\"Judge score distribution by dimension\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_xlabel(\"Dimension\")\n",
    "ax.set_ylim(0, 1)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcc493b",
   "metadata": {},
   "source": [
    "## Citation Coverage by Task Type\n",
    "Aggregate citation handling quality for each task class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ec8ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_stats = (\n",
    "    df.groupby(\"task_type\")\n",
    "    .agg(\n",
    "        examples=(\"task_id\", \"count\"),\n",
    "        mean_coverage=(\"mean_coverage\", \"mean\"),\n",
    "        median_coverage=(\"mean_coverage\", \"median\"),\n",
    "        avg_missing=(\"missing_citations\", \"mean\"),\n",
    "        avg_extra=(\"extra_citations\", \"mean\"),\n",
    "    )\n",
    "    .sort_values(\"mean_coverage\", ascending=False)\n",
    ")\n",
    "coverage_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb5333",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "coverage_stats[\"mean_coverage\"].plot(kind=\"bar\", ax=ax, color=\"#4c72b0\")\n",
    "ax.set_ylabel(\"Mean citation coverage\")\n",
    "ax.set_xlabel(\"Task type\")\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title(\"Citation coverage by task type\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a41c02",
   "metadata": {},
   "source": [
    "## Distribution of Missing Citations\n",
    "Visualise how often the model failed to cite required references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710ffbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "max_missing = df[\"missing_citations\"].max()\n",
    "bins = range(0, int(max_missing) + 2) if max_missing and max_missing > 0 else 5\n",
    "df[\"missing_citations\"].plot(kind=\"hist\", bins=bins, ax=ax, color=\"#dd8452\")\n",
    "ax.set_xlabel(\"Missing citations per example\")\n",
    "ax.set_ylabel(\"Example count\")\n",
    "ax.set_title(\"Distribution of missing citations\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c803dbe8",
   "metadata": {},
   "source": [
    "## Classification-Style Metrics\n",
    "Translate the citation outcomes into binary labels so we can compute accuracy metrics. An example counts as a success when it includes at least one citation and the judge did not flag missing or extra references. Use the threshold below to require a minimum citation coverage before calling it correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734246bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUCCESS_THRESHOLD = 0.10  # tweak as needed for stricter leniency\n",
    "df[\"citation_success\"] = (\n",
    "    (df[\"referenced_citations\"] > 0) & (df[\"missing_citations\"] == 0) & (df[\"extra_citations\"] == 0)\n",
    ")\n",
    "df[\"citation_score\"] = df[\"mean_coverage\"].fillna(0.0)\n",
    "\n",
    "y_true = df[\"citation_success\"].astype(int)\n",
    "y_score = df[\"citation_score\"]\n",
    "y_pred = (y_score >= SUCCESS_THRESHOLD).astype(int)\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "    \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "    \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "    \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "}\n",
    "\n",
    "try:\n",
    "    metrics[\"roc_auc\"] = roc_auc_score(y_true, y_score)\n",
    "except ValueError:\n",
    "    metrics[\"roc_auc\"] = float(\"nan\")\n",
    "\n",
    "metrics_series = pd.Series(metrics, name=\"value\")\n",
    "non_zero_metrics = metrics_series[~np.isclose(metrics_series, 0.0, atol=1e-6)].dropna()\n",
    "\n",
    "if non_zero_metrics.empty:\n",
    "    print(\"No non-zero metrics to report at the current threshold.\")\n",
    "else:\n",
    "    display(non_zero_metrics.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39456ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = pd.crosstab(\n",
    "    y_true, y_pred, rownames=[\"Actual success\"], colnames=[\"Predicted success\"], dropna=False\n",
    ")\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f0cf7e",
   "metadata": {},
   "source": [
    "## Judge Score Signals\n",
    "Inspect the raw judge score payloads to see which structured fields (if any) were populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4c3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_keys = sorted(\n",
    "    {key for scores in df[\"judge_scores\"] if isinstance(scores, dict) for key in scores.keys()}\n",
    ")\n",
    "score_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6e6492",
   "metadata": {},
   "source": [
    "## Lowest-Coverage Examples\n",
    "Review the examples with the weakest citation coverage to understand qualitative failure modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09ce8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_show = [\n",
    "    \"task_id\",\n",
    "    \"task_type\",\n",
    "    \"mean_coverage\",\n",
    "    \"missing_citations\",\n",
    "    \"extra_citations\",\n",
    "    \"model_answer\",\n",
    "]\n",
    "df.sort_values(\"mean_coverage\").head(5)[columns_to_show]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
