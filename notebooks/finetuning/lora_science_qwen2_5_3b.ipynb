{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e8c1bfa",
   "metadata": {
    "id": "2e8c1bfa"
   },
   "source": [
    "# LoRA Fine-Tuning: 3B Instruction-Only Model\n",
    "\n",
    "This Kaggle-ready workflow fine-tunes `Qwen/Qwen2.5-3B-Instruct` with LoRA adapters on an offline dataset **WITHOUT RAG contexts** for the 3B experiment series.\n",
    "\n",
    "**Important**: This notebook trains the model WITHOUT RAG contexts (instruction only). For the complete 6-condition experiment, you would need TWO models:\n",
    "- **Instruction-only model** (this notebook): Used for FT-only and FT+RAG instruction-only conditions\n",
    "- **RAG-trained model**: Trained WITH RAG contexts, used for RAG-trained FT-only and RAG-trained FT+RAG conditions\n",
    "\n",
    "**Note**: This notebook is for future 3B experiments. For current 0.5B experiments, use `lora_science_v1_instruction_only.ipynb` (instruction-only) and `lora_science_v1.ipynb` (RAG-trained)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d367550",
   "metadata": {
    "id": "8d367550"
   },
   "source": [
    "### 0. Runtime checklist\n",
    "* In the Kaggle notebook settings, enable a GPU accelerator (T4 minimum; L4/A100 improve throughput) and turn on internet access if you plan to download artifacts.\n",
    "* Kaggle provides 30 GB of RAM and ample disk under `/kaggle/working`; still monitor VRAM usage (14 GB on a T4 with these hyperparameters) and adjust max sequence length or gradient accumulation if you encounter OOM errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4998778d",
   "metadata": {
    "id": "4998778d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bee5ff",
   "metadata": {
    "id": "d4bee5ff"
   },
   "source": [
    "### 1. Install Python dependencies\n",
    "We pin versions compatible with Transformers 4.46+ and TRL's SFTTrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5ed237",
   "metadata": {
    "id": "0d5ed237",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade pip\n",
    "!pip install --quiet \"transformers>=4.46.0\" \"accelerate>=0.34.0\" \"datasets>=3.0.0\" peft trl bitsandbytes sentencepiece evaluate huggingface_hub pynvml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f190f",
   "metadata": {
    "id": "fe2f190f"
   },
   "source": [
    "### 2. Authenticate (optional but recommended)\n",
    "Set Hugging Face and GitHub tokens if you plan to pull private assets or push adapters. Tokens are stored in-memory only for this runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f0c6e9",
   "metadata": {
    "id": "99f0c6e9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from getpass import getpass\n",
    "\n",
    "\n",
    "def _get_kaggle_secret(key: str) -> str | None:\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "        secret = UserSecretsClient().get_secret(key)\n",
    "        if secret:\n",
    "            return secret.strip() or None\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\") or _get_kaggle_secret(\"HF_TOKEN\")\n",
    "if hf_token is None:\n",
    "    entered = getpass(\"Enter Hugging Face token (leave blank to skip): \")\n",
    "    hf_token = entered.strip() or None\n",
    "if hf_token:\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "    subprocess.run(\n",
    "        [\"huggingface-cli\", \"login\", \"--token\", hf_token, \"--add-to-git-credential\"],\n",
    "        check=False,\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping Hugging Face login.\")\n",
    "\n",
    "if os.environ.get(\"GITHUB_TOKEN\") is None:\n",
    "    gh_token = _get_kaggle_secret(\"GITHUB_TOKEN\")\n",
    "    if gh_token is None:\n",
    "        gh_token = getpass(\n",
    "            \"Enter GitHub token for private repo access (leave blank to skip): \"\n",
    "        ).strip()\n",
    "    if gh_token:\n",
    "        os.environ[\"GITHUB_TOKEN\"] = gh_token\n",
    "        print(\"Stored GitHub token in this session.\")\n",
    "    else:\n",
    "        print(\"Skipping GitHub token setup. Upload the dataset manually if download fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa2a951",
   "metadata": {
    "id": "cfa2a951"
   },
   "source": [
    "### 3. Configure working directories (Kaggle-friendly)\n",
    "Kaggle automatically exposes datasets under `/kaggle/input` and persists notebook outputs under `/kaggle/working`. Use the cell below to point the workflow at those locations, or override the paths if you run this notebook elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3fe491",
   "metadata": {
    "id": "fc3fe491",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "IN_KAGGLE = bool(os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"))\n",
    "DEFAULT_BASE_DIR = Path(\"/kaggle/working\") / \"beyond-the-cutoff\" if IN_KAGGLE else Path(\"/content\")\n",
    "BASE_DIR = Path(os.environ.get(\"BTC_BASE_DIR\", DEFAULT_BASE_DIR))\n",
    "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration: Update these for your 3B experiment\n",
    "RUN_ID = \"cog-psych-2025-run01\"  # kaggle changes _ (underscores) for - (hyphens) in datasets\n",
    "EXPERIMENT_NAME = \"lora_science_3b_instruction_only\"  # Updated to match experiment naming\n",
    "DATASET_FILENAME = \"offline_dataset.jsonl\"  # Use standard offline dataset name\n",
    "SPLIT_SEED = 20251107\n",
    "SPLIT_PREFIX = RUN_ID\n",
    "\n",
    "KAGGLE_DATASET_SUBDIR = os.environ.get(\"BTC_KAGGLE_DATASET_SUBDIR\", RUN_ID)\n",
    "DATASET_SOURCE_CANDIDATE = None\n",
    "if IN_KAGGLE:\n",
    "    kaggle_input_root = Path(\"/kaggle/input\")\n",
    "    kaggle_candidates = [\n",
    "        kaggle_input_root / DATASET_FILENAME,\n",
    "        kaggle_input_root / RUN_ID / DATASET_FILENAME,\n",
    "        kaggle_input_root / KAGGLE_DATASET_SUBDIR / DATASET_FILENAME,\n",
    "    ]\n",
    "    for path in kaggle_candidates:\n",
    "        if path.exists():\n",
    "            DATASET_SOURCE_CANDIDATE = path\n",
    "            break\n",
    "\n",
    "print(f\"Working directory base: {BASE_DIR}\")\n",
    "if IN_KAGGLE:\n",
    "    print(\"Detected Kaggle runtime.\")\n",
    "    if DATASET_SOURCE_CANDIDATE:\n",
    "        print(f\"Found dataset candidate at {DATASET_SOURCE_CANDIDATE}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"Attach the dataset under /kaggle/input or set BTC_KAGGLE_DATASET_SUBDIR/DATASET_SOURCE_PATH.\"\n",
    "        )\n",
    "else:\n",
    "    print(\"Running outside Kaggle; override BTC_BASE_DIR if needed.\")\n",
    "\n",
    "print(f\"Run ID: {RUN_ID} | Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"Dataset file: {DATASET_FILENAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bad632",
   "metadata": {
    "id": "f5bad632"
   },
   "source": [
    "### 4. Retrieve the offline dataset\n",
    "Add the `cog_psych_offline_dataset.jsonl` file as an attached Kaggle Dataset (for example under `/kaggle/input/cog_psych_2025_run01/`). If internet access is enabled, the cell below can also download directly from GitHub; otherwise the Kaggle attachment is required. NOTE: kaggle changes _ (underscores) for - (hyphens) in DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fe97ec",
   "metadata": {
    "id": "a6fe97ec",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(BASE_DIR) / \"data\" / RUN_ID\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATASET_PATH = DATA_DIR / DATASET_FILENAME\n",
    "\n",
    "if DATASET_PATH.exists():\n",
    "    print(f\"Dataset already available: {DATASET_PATH}\")\n",
    "else:\n",
    "    dataset_source_override = os.environ.get(\"BTC_DATASET_SOURCE\")\n",
    "    candidate_paths = []\n",
    "    if dataset_source_override:\n",
    "        candidate_paths.append(Path(dataset_source_override))\n",
    "    if \"DATASET_SOURCE_CANDIDATE\" in globals() and DATASET_SOURCE_CANDIDATE:\n",
    "        candidate_paths.append(Path(DATASET_SOURCE_CANDIDATE))\n",
    "\n",
    "    copied = False\n",
    "    for candidate in candidate_paths:\n",
    "        if candidate.exists():\n",
    "            shutil.copy(candidate, DATASET_PATH)\n",
    "            print(f\"Copied dataset from {candidate} -> {DATASET_PATH}\")\n",
    "            copied = True\n",
    "            break\n",
    "\n",
    "    if not copied:\n",
    "        import requests\n",
    "\n",
    "        headers = {}\n",
    "        github_token = os.environ.get(\"GITHUB_TOKEN\")\n",
    "        if github_token:\n",
    "            headers[\"Authorization\"] = f\"token {github_token}\"\n",
    "        url = (\n",
    "            \"https://raw.githubusercontent.com/ignaciolinari/beyond-the-cutoff/main/\"\n",
    "            f\"evaluation/datasets/{DATASET_FILENAME}\"\n",
    "        )\n",
    "        response = requests.get(url, headers=headers, timeout=60)\n",
    "        if response.status_code == 200:\n",
    "            DATASET_PATH.write_text(response.text, encoding=\"utf-8\")\n",
    "            print(f\"Downloaded dataset to {DATASET_PATH}\")\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                \"Failed to retrieve dataset. Attach it as a Kaggle dataset or set BTC_DATASET_SOURCE.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52cc6ed",
   "metadata": {
    "id": "b52cc6ed"
   },
   "source": [
    "### 5. Create deterministic train/val/test splits\n",
    "We stratify by paper and task type using seed `20251107` to stay aligned with the cognitive psychology pipeline plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787565a6",
   "metadata": {
    "id": "787565a6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def load_examples(path: Path) -> list[dict]:\n",
    "    raw = path.read_text(encoding=\"utf-8\").strip().splitlines()\n",
    "    return [json.loads(line) for line in raw if line]\n",
    "\n",
    "\n",
    "def extract_paper_id(example: dict) -> str:\n",
    "    meta = example.get(\"metadata\") or {}\n",
    "    if isinstance(meta, dict):\n",
    "        candidate = meta.get(\"source_path\") or meta.get(\"paper_id\")\n",
    "        if candidate:\n",
    "            return Path(str(candidate)).stem\n",
    "    sources = example.get(\"sources\") or []\n",
    "    if sources:\n",
    "        return Path(str(sources[0])).stem\n",
    "    rag = example.get(\"rag\") or {}\n",
    "    retrieved = rag.get(\"retrieved\") or []\n",
    "    if retrieved:\n",
    "        first = retrieved[0]\n",
    "        if isinstance(first, dict) and first.get(\"source_path\"):\n",
    "            return Path(str(first[\"source_path\"])).stem\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "examples = load_examples(DATASET_PATH)\n",
    "print(f\"Loaded {len(examples)} examples\")\n",
    "\n",
    "groups = defaultdict(list)\n",
    "for example in examples:\n",
    "    key = (extract_paper_id(example), example.get(\"task_type\"))\n",
    "    groups[key].append(example)\n",
    "\n",
    "buckets = list(groups.values())\n",
    "rng = random.Random(SPLIT_SEED)\n",
    "rng.shuffle(buckets)\n",
    "\n",
    "target_counts = {\n",
    "    \"train\": int(round(0.70 * len(examples))),\n",
    "    \"val\": int(round(0.15 * len(examples))),\n",
    "}\n",
    "target_counts[\"test\"] = len(examples) - target_counts[\"train\"] - target_counts[\"val\"]\n",
    "splits = {\"train\": [], \"val\": [], \"test\": []}\n",
    "\n",
    "for bucket in buckets:\n",
    "    remaining = {split: target_counts[split] - len(splits[split]) for split in splits}\n",
    "    target_split = max(remaining, key=lambda split: remaining[split])\n",
    "    if remaining[target_split] <= 0:\n",
    "        target_split = \"train\"\n",
    "    splits[target_split].extend(bucket)\n",
    "\n",
    "SPLIT_DIR = DATA_DIR / \"splits\"\n",
    "SPLIT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "for split, rows in splits.items():\n",
    "    out_path = SPLIT_DIR / f\"{SPLIT_PREFIX}_{split}.jsonl\"\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as handle:\n",
    "        for row in rows:\n",
    "            handle.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"{split:>5}: {len(rows):>3} examples → {out_path}\")\n",
    "\n",
    "print(f\"Used stratification seed {SPLIT_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa65f7c6",
   "metadata": {
    "id": "aa65f7c6"
   },
   "source": [
    "### 6. Load dataset into Hugging Face `DatasetDict`\n",
    "We keep the raw fields and delegate prompt construction to the trainer's formatting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140f074",
   "metadata": {
    "id": "1140f074",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "def read_split(split: str) -> Dataset:\n",
    "    path = SPLIT_DIR / f\"{SPLIT_PREFIX}_{split}.jsonl\"\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as handle:\n",
    "        for line in handle:\n",
    "            if line.strip():\n",
    "                rows.append(json.loads(line))\n",
    "    return Dataset.from_list(rows)\n",
    "\n",
    "\n",
    "dataset = DatasetDict({split: read_split(split) for split in [\"train\", \"val\", \"test\"]})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261befab",
   "metadata": {
    "id": "261befab"
   },
   "source": [
    "### 7. Initialize tokenizer, model, and LoRA config\n",
    "We keep the base model in float16 unless the GPU supports bfloat16, and target attention + MLP projections for LoRA adapters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42caaf48",
   "metadata": {
    "id": "42caaf48",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "supports_cuda = torch.cuda.is_available()\n",
    "compute_capability = torch.cuda.get_device_capability(0)[0] if supports_cuda else None\n",
    "prefer_bf16 = bool(supports_cuda and compute_capability is not None and compute_capability >= 8)\n",
    "model_dtype = torch.bfloat16 if prefer_bf16 else torch.float16\n",
    "\n",
    "\n",
    "def build_base_model() -> AutoModelForCausalLM:\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=model_dtype,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    base_model.config.use_cache = False\n",
    "    return base_model\n",
    "\n",
    "\n",
    "model = build_base_model()\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(f\"Loaded model on {model.device} with dtype {model_dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261befab",
   "metadata": {
    "id": "261befab"
   },
   "source": [
    "### 8. Define prompt formatting and trainer\n",
    "\n",
    "**INSTRUCTION-ONLY MODE**: We use only the instruction (no RAG contexts) in the user turn. This trains the model to answer questions based on its knowledge alone, matching the FT-only evaluation setup. The same model will be used for both FT-only and RAG+FT evaluation (mirroring the 0.5B experiment design).\n",
    "\n",
    "**Important**: The user message format matches `_build_instruction_only_prompt()` from the evaluation runner to ensure training/evaluation consistency. The prompt includes the instruction text wrapped in the same format used during evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42caaf48",
   "metadata": {
    "id": "42caaf48",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections.abc import Mapping, Sequence\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "\n",
    "def _get_batch_value(column, index):\n",
    "    if column is None:\n",
    "        return None\n",
    "    if isinstance(column, Mapping):\n",
    "        return {key: _get_batch_value(value, index) for key, value in column.items()}\n",
    "    if isinstance(column, Sequence) and not isinstance(column, str | bytes):\n",
    "        return column[index] if len(column) > index else None\n",
    "    return column\n",
    "\n",
    "\n",
    "def build_user_message(\n",
    "    instruction: str,\n",
    "    rag_entry: dict | None,\n",
    "    contexts_fallback: Sequence[str] | None,\n",
    ") -> str:\n",
    "    # INSTRUCTION-ONLY MODE: Return only instruction, ignore contexts\n",
    "    # This trains the model to answer without RAG contexts\n",
    "    # Format matches evaluation runner's _build_instruction_only_prompt() for consistency\n",
    "    instruction_text = instruction.strip()\n",
    "    if not instruction_text:\n",
    "        return \"\"\n",
    "    return (\n",
    "        \"You are a research paper assistant. Answer the following question based on your knowledge.\\n\\n\"\n",
    "        f\"Question: {instruction_text}\\n\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "\n",
    "def format_example(example: dict) -> dict[str, str]:\n",
    "    instruction = (example[\"instruction\"] or \"\").strip()\n",
    "    rag_item = example.get(\"rag\")\n",
    "    contexts_item = example.get(\"contexts\")\n",
    "    user_content = build_user_message(instruction, rag_item, contexts_item)\n",
    "    assistant_reply = (example.get(\"expected_response\") or \"\").strip()\n",
    "    # Use chat template format (required for HuggingFace models)\n",
    "    # but user content matches evaluation format for consistency\n",
    "    # System message is minimal since instructions are in user content\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a research paper assistant.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_reply},\n",
    "    ]\n",
    "    rendered = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    if tokenizer.eos_token:\n",
    "        rendered += tokenizer.eos_token\n",
    "    return {\"text\": rendered}\n",
    "\n",
    "\n",
    "output_root = Path(BASE_DIR) / \"outputs\" / EXPERIMENT_NAME\n",
    "output_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "model_dtype = globals().get(\"model_dtype\", getattr(model, \"dtype\", torch.float16))\n",
    "supports_cuda = torch.cuda.is_available()\n",
    "prefer_bf16 = globals().get(\"prefer_bf16\", supports_cuda and model_dtype == torch.bfloat16)\n",
    "fp16_flag = supports_cuda and model_dtype == torch.float16\n",
    "bf16_flag = supports_cuda and prefer_bf16\n",
    "max_seq_length = 2048\n",
    "tokenizer.model_max_length = max_seq_length\n",
    "\n",
    "\n",
    "if tokenizer.eos_token is None or not isinstance(tokenizer.eos_token, str):\n",
    "    tokenizer.eos_token = \"<|endoftext|>\"\n",
    "\n",
    "\n",
    "if hasattr(model, \"peft_config\"):\n",
    "    model = build_base_model()\n",
    "\n",
    "\n",
    "formatted_dataset = dataset.map(format_example, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=str(output_root),\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=fp16_flag,\n",
    "    bf16=bf16_flag,\n",
    "    max_grad_norm=1.0,\n",
    "    report_to=\"none\",\n",
    "    packing=False,\n",
    "    per_device_eval_batch_size=1,\n",
    "    eval_accumulation_steps=1,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset[\"train\"],\n",
    "    eval_dataset=formatted_dataset[\"val\"],\n",
    "    peft_config=lora_config,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae6a80",
   "metadata": {
    "id": "aaae6a80"
   },
   "source": [
    "### 9. Fine-tune the model\n",
    "Training typically finishes in 30 minutes on a T4 due to the compact dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731bf29e",
   "metadata": {
    "id": "731bf29e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_batch_size = training_args.per_device_train_batch_size * max(\n",
    "    1, training_args.gradient_accumulation_steps\n",
    ")\n",
    "print(\n",
    "    f\"Starting LoRA fine-tuning on {len(formatted_dataset['train'])} training examples \"\n",
    "    f\"({train_batch_size} effective batch size) for {training_args.num_train_epochs} epochs.\"\n",
    ")\n",
    "print(\n",
    "    f\"Evaluation enabled: {training_args.eval_strategy != 'no'} | \"\n",
    "    f\"Logging every {training_args.logging_steps} steps | \"\n",
    "    f\"Gradient checkpointing: {training_args.gradient_checkpointing}\"\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(\"Training finished.\")\n",
    "trainer.log_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "if train_result.metrics:\n",
    "    print(\"Train metrics:\")\n",
    "    for key, value in train_result.metrics.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "eval_metrics = None\n",
    "if training_args.eval_strategy != \"no\":\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    trainer.log_metrics(\"eval\", eval_metrics)\n",
    "    trainer.save_metrics(\"eval\", eval_metrics)\n",
    "    if eval_metrics:\n",
    "        print(\"Eval metrics:\")\n",
    "        for key, value in eval_metrics.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"Evaluation skipped (eval_strategy='no').\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9354cecf",
   "metadata": {
    "id": "9354cecf"
   },
   "source": [
    "#### Plot training loss curve\n",
    "Use this after training to confirm the optimizer is behaving as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146990de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "loss_history = [\n",
    "    {\"step\": entry[\"step\"], \"loss\": entry[\"loss\"]}\n",
    "    for entry in trainer.state.log_history\n",
    "    if \"loss\" in entry and \"step\" in entry\n",
    "]\n",
    "\n",
    "if not loss_history:\n",
    "    print(\"No loss values logged yet. Run the training cell first or raise logging verbosity.\")\n",
    "else:\n",
    "    loss_df = pd.DataFrame(loss_history).drop_duplicates(subset=\"step\").sort_values(\"step\")\n",
    "    display(loss_df.tail())\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "    ax.plot(loss_df[\"step\"], loss_df[\"loss\"], marker=\"o\", linewidth=1.5, markersize=3)\n",
    "    ax.set_xlabel(\"Global step\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Training Loss vs. Step\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea0dce1",
   "metadata": {
    "id": "7ea0dce1"
   },
   "source": [
    "### 10. Quick validation sanity check\n",
    "Generate answers for a few validation samples to verify the adapter behaviour before exporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a02ba5",
   "metadata": {
    "id": "57a02ba5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "\n",
    "def preview_response(example: dict, max_new_tokens: int = 256) -> str:\n",
    "    user_text = build_user_message(\n",
    "        example[\"instruction\"], example.get(\"rag\"), example.get(\"contexts\")\n",
    "    )\n",
    "    # Match training format: minimal system message, user content includes instructions\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a research paper assistant.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": user_text},\n",
    "    ]\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    encoded = tokenizer(\n",
    "        prompt_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    encoded = {key: value.to(model.device) for key, value in encoded.items()}\n",
    "    if encoded[\"input_ids\"].shape[-1] == tokenizer.model_max_length:\n",
    "        print(\n",
    "            \"Prompt truncated to fit model_max_length; consider tightening contexts if this occurs often.\"\n",
    "        )\n",
    "    gen_config = GenerationConfig(\n",
    "        do_sample=False,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(**encoded, generation_config=gen_config)\n",
    "    response_ids = generated[0, encoded[\"input_ids\"].shape[-1] :]\n",
    "    return tokenizer.decode(response_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "for example in dataset[\"val\"].select(range(min(3, len(dataset[\"val\"])))):\n",
    "    print(\"Instruction:\", example[\"instruction\"])\n",
    "    print(\"Ground truth:\", example.get(\"expected_response\", \"\").strip())\n",
    "    print(\"Model output:\", preview_response(example))\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e03831",
   "metadata": {
    "id": "22e03831"
   },
   "source": [
    "### 11. Save adapters and tokenizer\n",
    "Store artifacts under `outputs/adapters/lora_science_3b_instruction_only`. Download them from Kaggle or sync to your preferred remote storage as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6687d1",
   "metadata": {
    "id": "8c6687d1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "adapter_dir = output_root / \"adapters\"\n",
    "adapter_dir.mkdir(parents=True, exist_ok=True)\n",
    "adapter_path = adapter_dir / EXPERIMENT_NAME\n",
    "trainer.model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "print(f\"Saved LoRA adapter to {adapter_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd0530b",
   "metadata": {
    "id": "efd0530b"
   },
   "source": [
    "### 12. Package artifacts\n",
    "Compress the adapter, training args, and logs so you can download them from Kaggle or sync back to the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b94954",
   "metadata": {
    "id": "36b94954",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# archive_stem = output_root.parent / f\"{EXPERIMENT_NAME}_artifacts\"\n",
    "# zip_target = archive_stem.with_suffix(\".zip\")\n",
    "# if zip_target.exists():\n",
    "#     zip_target.unlink()\n",
    "# zip_path = shutil.make_archive(str(archive_stem), \"zip\", root_dir=output_root)\n",
    "# print(f\"Packed artifacts at {zip_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d182a42",
   "metadata": {
    "id": "1d182a42"
   },
   "source": [
    "### 13. Next steps\n",
    "1. **Persist training metadata** → run the next cell to capture seed `20251107`, the key hyperparameters, and recent metrics in `outputs/adapters/lora_science_3b_instruction_only/EXPERIMENT_METADATA.json`.\n",
    "2. **Materialize merged weights** → execute the following cell to merge the LoRA adapter into the base model and save a ready-to-quantize checkpoint under `outputs/lora_science_3b_instruction_only/merged_full_model`.\n",
    "3. **Quantize for Ollama** → use the provided CLI snippet to convert the merged weights to GGUF via `llama.cpp`'s `convert-hf-to-gguf.py`, then register the artifact with Ollama.\n",
    "4. **Re-benchmark** → rerun `scripts/evaluate_models.py` with the new Ollama tag to compare against the `qwen2_5-3b-instruct-q4_K_M` RAG control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d859e7",
   "metadata": {
    "id": "f3d859e7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import peft\n",
    "import transformers\n",
    "import trl\n",
    "from packaging.version import Version\n",
    "\n",
    "metadata_target = output_root / \"adapters\" / EXPERIMENT_NAME / \"EXPERIMENT_METADATA.json\"\n",
    "metadata_target.parent.mkdir(parents=True, exist_ok=True)\n",
    "train_result_obj = locals().get(\"train_result\")\n",
    "\n",
    "training_summary = {\n",
    "    \"seed\": SPLIT_SEED,\n",
    "    \"timestamp_utc\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
    "    \"base_model\": model_name,\n",
    "    \"adapter_dir\": str(output_root / \"adapters\" / EXPERIMENT_NAME),\n",
    "    \"output_dir\": str(output_root),\n",
    "    \"hyperparameters\": {\n",
    "        \"num_train_epochs\": training_args.num_train_epochs,\n",
    "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"weight_decay\": training_args.weight_decay,\n",
    "        \"lr_scheduler_type\": training_args.lr_scheduler_type,\n",
    "        \"warmup_ratio\": training_args.warmup_ratio,\n",
    "        \"max_seq_length\": max_seq_length,\n",
    "    },\n",
    "    \"optimizer_steps\": trainer.state.global_step,\n",
    "    \"train_loss\": next(\n",
    "        (entry.get(\"loss\") for entry in reversed(trainer.state.log_history) if \"loss\" in entry),\n",
    "        None,\n",
    "    ),\n",
    "    \"final_metrics\": train_result_obj.metrics if train_result_obj is not None else {},\n",
    "    \"libraries\": {\n",
    "        \"transformers\": Version(transformers.__version__).base_version,\n",
    "        \"trl\": Version(trl.__version__).base_version,\n",
    "        \"peft\": Version(peft.__version__).base_version,\n",
    "    },\n",
    "}\n",
    "\n",
    "metadata_target.write_text(\n",
    "    json.dumps(training_summary, indent=2, sort_keys=True) + \"\\n\", encoding=\"utf-8\"\n",
    ")\n",
    "print(f\"Wrote metadata to {metadata_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd184a",
   "metadata": {
    "id": "47fd184a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "output_root = Path(BASE_DIR) / \"outputs\" / EXPERIMENT_NAME\n",
    "adapter_dir = output_root / \"adapters\"\n",
    "adapter_path = adapter_dir / EXPERIMENT_NAME\n",
    "\n",
    "merged_output_dir = output_root / \"merged_full_model\"\n",
    "merged_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Merging adapter into the base model…\")\n",
    "\n",
    "# Free up memory by deleting the original model and clearing CUDA cache\n",
    "if \"model\" in globals():\n",
    "    del model\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "offload_dir = Path(BASE_DIR) / \"tmp_merge_offload\"\n",
    "offload_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "merge_dtype = torch.float16\n",
    "merged_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    adapter_path,\n",
    "    torch_dtype=merge_dtype,\n",
    "    device_map={\"\": \"cpu\"},\n",
    "    low_cpu_mem_usage=True,\n",
    "    offload_folder=str(offload_dir),\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "merged_model.save_pretrained(merged_output_dir)\n",
    "tokenizer.save_pretrained(merged_output_dir)\n",
    "print(f\"Merged checkpoint written to {merged_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e5b9c4",
   "metadata": {
    "id": "76e5b9c4"
   },
   "source": [
    "#### Quantize and evaluate\n",
    "Run the commands below from your local checkout once the merged checkpoint is synced down:\n",
    "```bash\n",
    "# Convert merged HF weights to GGUF\n",
    "python /path/to/llama.cpp/convert-hf-to-gguf.py \\\n",
    "  --model-dir outputs/lora_science_3b_instruction_only/merged_full_model \\\n",
    "  --outfile outputs/lora_science_3b_instruction_only/merged_full_model/Qwen2.5-3B-lora_science_3b_instruction_only.Q4_K_M.gguf \\\n",
    "  --data-type q4_K_M\n",
    "\n",
    "# Register with Ollama using instruction-only Modelfile\n",
    "# This same model will be used for both FT-only and RAG+FT evaluation\n",
    "ollama create lora_science_3b_instruction_only -f ollama/Modelfile.instruction_only\n",
    "ollama push lora_science_3b_instruction_only\n",
    "\n",
    "# Create a 3B comparison plan first (e.g., compare_3b_experiments.yaml)\n",
    "# based on configs/evaluation/compare_0p5b_experiments.yaml, then run:\n",
    "python scripts/compare_models.py \\\n",
    "  --plan configs/evaluation/compare_3b_experiments.yaml \\\n",
    "  --output evaluation/results/comparison_report_3b.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0867b39f",
   "metadata": {
    "id": "0867b39f"
   },
   "source": [
    "### 14. Re-package artifacts\n",
    "Create a fresh archive after merging and metadata updates so downstream steps stay in sync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd1da73",
   "metadata": {
    "id": "3bd1da73",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "archive_stem = output_root.parent / f\"{EXPERIMENT_NAME}_artifacts_postmerge\"\n",
    "zip_target = archive_stem.with_suffix(\".zip\")\n",
    "if zip_target.exists():\n",
    "    zip_target.unlink()\n",
    "zip_path = shutil.make_archive(str(archive_stem), \"zip\", root_dir=output_root)\n",
    "print(f\"Packed updated artifacts at {zip_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8703600,
     "sourceId": 13685569,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
