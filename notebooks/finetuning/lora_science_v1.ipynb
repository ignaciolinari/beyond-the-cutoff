{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ede252e",
   "metadata": {
    "id": "9ede252e"
   },
   "source": [
    "# LoRA Fine-Tuning: lora_science_v1\n",
    "\n",
    "This Colab-ready workflow fine-tunes `Qwen/Qwen2-0.5B-Instruct` with LoRA adapters on the 19-document offline dataset so we can benchmark the `lora_science_v1` experiment against the `rag_baseline_v1` control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6edeb68",
   "metadata": {
    "id": "d6edeb68"
   },
   "source": [
    "### 0. Runtime checklist\n",
    "* Select a Colab runtime with GPU (T4 preferred) before running any code.\n",
    "* Keep an eye on VRAM usage (~16 GB on T4). Reduce sequence length or increase gradient accumulation if memory errors appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d0828",
   "metadata": {
    "id": "776d0828"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b9ca47",
   "metadata": {
    "id": "54b9ca47"
   },
   "source": [
    "### 1. Install Python dependencies\n",
    "We pin versions compatible with Transformers 4.45+ and TRL's SFTTrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa52980",
   "metadata": {
    "id": "2fa52980"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade pip\n",
    "!pip install --quiet \"transformers>=4.45.0\" \"accelerate>=0.33.0\" \"datasets>=3.0.0\" peft trl bitsandbytes sentencepiece evaluate huggingface_hub pynvml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b421ec84",
   "metadata": {
    "id": "b421ec84"
   },
   "source": [
    "### 2. Authenticate (optional but recommended)\n",
    "Set Hugging Face and GitHub tokens if you plan to pull private assets or push adapters. Tokens are stored in-memory only for this runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b309bd8",
   "metadata": {
    "id": "7b309bd8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from getpass import getpass\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token is None:\n",
    "    entered = getpass(\"Enter Hugging Face token (leave blank to skip): \")\n",
    "    hf_token = entered.strip() or None\n",
    "if hf_token:\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "    subprocess.run(\n",
    "        [\"huggingface-cli\", \"login\", \"--token\", hf_token, \"--add-to-git-credential\"], check=False\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping Hugging Face login.\")\n",
    "\n",
    "if os.environ.get(\"GITHUB_TOKEN\") is None:\n",
    "    gh_token = getpass(\"Enter GitHub token for private repo access (leave blank to skip): \")\n",
    "    if gh_token.strip():\n",
    "        os.environ[\"GITHUB_TOKEN\"] = gh_token.strip()\n",
    "        print(\"Stored GitHub token in this session.\")\n",
    "    else:\n",
    "        print(\"Skipping GitHub token setup. Upload the dataset manually if download fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159b6149",
   "metadata": {
    "id": "159b6149"
   },
   "source": [
    "### 3. (Optional) Mount Google Drive\n",
    "Use Drive if you want automatic persistence of adapters, logs, or dataset snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807063f",
   "metadata": {
    "id": "a807063f"
   },
   "outputs": [],
   "source": [
    "USE_DRIVE = True  # flip to True if you want to mount Drive\n",
    "if USE_DRIVE:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "    BASE_DIR = \"/content/drive/MyDrive/beyond-the-cutoff\"\n",
    "else:\n",
    "    BASE_DIR = \"/content\"\n",
    "print(f\"Working directory base: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9492ed8",
   "metadata": {
    "id": "e9492ed8"
   },
   "source": [
    "### 4. Retrieve the offline dataset\n",
    "We mirror `evaluation/datasets/offline_dataset.jsonl`. If the repo is private and no token is provided, upload the file manually to `/content/data/offline_eval/offline_dataset.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5d8c9",
   "metadata": {
    "id": "46b5d8c9"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(BASE_DIR) / \"data\" / \"offline_eval\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATASET_PATH = DATA_DIR / \"offline_dataset.jsonl\"\n",
    "\n",
    "if DATASET_PATH.exists():\n",
    "    print(f\"Dataset already available: {DATASET_PATH}\")\n",
    "else:\n",
    "    import requests\n",
    "\n",
    "    headers = {}\n",
    "    github_token = os.environ.get(\"GITHUB_TOKEN\")\n",
    "    if github_token:\n",
    "        headers[\"Authorization\"] = f\"token {github_token}\"\n",
    "    url = \"https://raw.githubusercontent.com/ignaciolinari/beyond-the-cutoff/main/evaluation/datasets/offline_dataset.jsonl\"\n",
    "    response = requests.get(url, headers=headers, timeout=60)\n",
    "    if response.status_code == 200:\n",
    "        DATASET_PATH.write_text(response.text, encoding=\"utf-8\")\n",
    "        print(f\"Downloaded dataset to {DATASET_PATH}\")\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"Failed to download dataset. Upload the file manually and rerun this cell.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a001ed2",
   "metadata": {
    "id": "7a001ed2"
   },
   "source": [
    "### 5. Create deterministic train/val/test splits\n",
    "We stratify by paper and task type using seed `20251101` to match the model adaptation plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d6a30",
   "metadata": {
    "id": "f95d6a30"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def load_examples(path: Path) -> list[dict]:\n",
    "    raw = path.read_text(encoding=\"utf-8\").strip().splitlines()\n",
    "    return [json.loads(line) for line in raw if line]\n",
    "\n",
    "\n",
    "def extract_paper_id(example: dict) -> str:\n",
    "    meta = example.get(\"metadata\") or {}\n",
    "    if isinstance(meta, dict):\n",
    "        candidate = meta.get(\"source_path\") or meta.get(\"paper_id\")\n",
    "        if candidate:\n",
    "            return Path(str(candidate)).stem\n",
    "    sources = example.get(\"sources\") or []\n",
    "    if sources:\n",
    "        return Path(str(sources[0])).stem\n",
    "    rag = example.get(\"rag\") or {}\n",
    "    retrieved = rag.get(\"retrieved\") or []\n",
    "    if retrieved:\n",
    "        first = retrieved[0]\n",
    "        if isinstance(first, dict) and first.get(\"source_path\"):\n",
    "            return Path(str(first[\"source_path\"])).stem\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "examples = load_examples(DATASET_PATH)\n",
    "print(f\"Loaded {len(examples)} examples\")\n",
    "\n",
    "groups = defaultdict(list)\n",
    "for example in examples:\n",
    "    key = (extract_paper_id(example), example.get(\"task_type\"))\n",
    "    groups[key].append(example)\n",
    "\n",
    "buckets = list(groups.values())\n",
    "rng = random.Random(20251101)\n",
    "rng.shuffle(buckets)\n",
    "\n",
    "target_counts = {\n",
    "    \"train\": int(round(0.70 * len(examples))),\n",
    "    \"val\": int(round(0.15 * len(examples))),\n",
    "}\n",
    "target_counts[\"test\"] = len(examples) - target_counts[\"train\"] - target_counts[\"val\"]\n",
    "splits = {\"train\": [], \"val\": [], \"test\": []}\n",
    "\n",
    "for bucket in buckets:\n",
    "    remaining = {split: target_counts[split] - len(splits[split]) for split in splits}\n",
    "    target_split = max(remaining, key=lambda split: remaining[split])\n",
    "    if remaining[target_split] <= 0:\n",
    "        target_split = \"train\"\n",
    "    splits[target_split].extend(bucket)\n",
    "\n",
    "SPLIT_DIR = DATA_DIR / \"splits\"\n",
    "SPLIT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "for split, rows in splits.items():\n",
    "    out_path = SPLIT_DIR / f\"lora_science_v1_{split}.jsonl\"\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as handle:\n",
    "        for row in rows:\n",
    "            handle.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"{split:>5}: {len(rows):>2} examples → {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10890a97",
   "metadata": {
    "id": "10890a97"
   },
   "source": [
    "### 6. Load dataset into Hugging Face `DatasetDict`\n",
    "We keep the raw fields and delegate prompt construction to the trainer’s formatting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68899475",
   "metadata": {
    "id": "68899475"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "def read_split(split: str) -> Dataset:\n",
    "    path = SPLIT_DIR / f\"lora_science_v1_{split}.jsonl\"\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as handle:\n",
    "        for line in handle:\n",
    "            if line.strip():\n",
    "                rows.append(json.loads(line))\n",
    "    return Dataset.from_list(rows)\n",
    "\n",
    "\n",
    "dataset = DatasetDict({split: read_split(split) for split in [\"train\", \"val\", \"test\"]})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d280fb3",
   "metadata": {
    "id": "5d280fb3"
   },
   "source": [
    "### 7. Initialize tokenizer, model, and LoRA config\n",
    "We keep the base model in float16 and target attention + MLP projections for LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e94e923",
   "metadata": {
    "id": "5e94e923"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model_name = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "supports_cuda = torch.cuda.is_available()\n",
    "\n",
    "compute_capability = torch.cuda.get_device_capability(0)[0] if supports_cuda else None\n",
    "\n",
    "prefer_bf16 = bool(supports_cuda and compute_capability is not None and compute_capability >= 8)\n",
    "\n",
    "model_dtype = torch.bfloat16 if prefer_bf16 else torch.float16\n",
    "\n",
    "\n",
    "def build_base_model() -> AutoModelForCausalLM:\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=model_dtype,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    base_model.config.use_cache = False\n",
    "    return base_model\n",
    "\n",
    "\n",
    "model = build_base_model()\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(f\"Loaded model on {model.device} with dtype {model_dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09821262",
   "metadata": {
    "id": "09821262"
   },
   "source": [
    "### 8. Define prompt formatting and trainer\n",
    "We stitch instruction + retrieval contexts into the user turn and keep the ground-truth response as the assistant turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc1c52",
   "metadata": {
    "id": "14dc1c52"
   },
   "outputs": [],
   "source": [
    "from collections.abc import Mapping, Sequence\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "\n",
    "def _get_batch_value(column, index):\n",
    "    if column is None:\n",
    "        return None\n",
    "    if isinstance(column, Mapping):\n",
    "        return {key: _get_batch_value(value, index) for key, value in column.items()}\n",
    "    if isinstance(column, Sequence) and not isinstance(column, str | bytes):\n",
    "        return column[index] if len(column) > index else None\n",
    "    return column\n",
    "\n",
    "\n",
    "def build_user_message(\n",
    "    instruction: str,\n",
    "    rag_entry: dict | None,\n",
    "    contexts_fallback: Sequence[str] | None,\n",
    ") -> str:\n",
    "    contexts = []\n",
    "    if rag_entry and isinstance(rag_entry, dict):\n",
    "        contexts = rag_entry.get(\"contexts\") or []\n",
    "    if not contexts and contexts_fallback is not None:\n",
    "        contexts = contexts_fallback\n",
    "    processed = [str(ctx).strip() for ctx in (contexts or []) if ctx]\n",
    "    context_block = \"\\n\\n\".join(processed[:6])\n",
    "    parts = [instruction.strip()]\n",
    "    if context_block:\n",
    "        parts.append(\"Context:\\n\" + context_block)\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "\n",
    "def format_example(example: dict) -> dict[str, str]:\n",
    "    instruction = (example[\"instruction\"] or \"\").strip()\n",
    "    rag_item = example.get(\"rag\")\n",
    "    contexts_item = example.get(\"contexts\")\n",
    "    user_content = build_user_message(instruction, rag_item, contexts_item)\n",
    "    assistant_reply = (example.get(\"expected_response\") or \"\").strip()\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a scientific research assistant who answers with concise, \"\n",
    "                \"evidence-grounded prose and includes inline numeric citations like [1].\"\n",
    "            ),\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_reply},\n",
    "    ]\n",
    "    rendered = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    # Ensure EOS token is added here as packing is False\n",
    "    if tokenizer.eos_token:\n",
    "        rendered += tokenizer.eos_token\n",
    "    return {\"text\": rendered}\n",
    "\n",
    "\n",
    "output_root = Path(BASE_DIR) / \"outputs\" / \"lora_science_v1\"\n",
    "output_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "model_dtype = globals().get(\"model_dtype\", getattr(model, \"dtype\", torch.float16))\n",
    "supports_cuda = torch.cuda.is_available()\n",
    "prefer_bf16 = globals().get(\"prefer_bf16\", supports_cuda and model_dtype == torch.bfloat16)\n",
    "fp16_flag = supports_cuda and model_dtype == torch.float16\n",
    "bf16_flag = supports_cuda and prefer_bf16\n",
    "tokenizer.model_max_length = 1024\n",
    "\n",
    "\n",
    "# Ensure tokenizer.eos_token is a string\n",
    "if tokenizer.eos_token is None or not isinstance(tokenizer.eos_token, str):\n",
    "    tokenizer.eos_token = \"<|endoftext|>\"  # Set a common EOS token if it's not already a string\n",
    "\n",
    "\n",
    "# Make sure we do not stack multiple adapters on the same model instance\n",
    "if hasattr(model, \"peft_config\"):\n",
    "    model = build_base_model()\n",
    "\n",
    "\n",
    "# Apply formatting to the dataset before passing to SFTTrainer\n",
    "formatted_dataset = dataset.map(format_example, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=str(output_root),\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=fp16_flag,\n",
    "    bf16=bf16_flag,\n",
    "    max_grad_norm=1.0,\n",
    "    report_to=\"none\",\n",
    "    packing=False,  # Keep packing=False as we formatted the text ourselves\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset[\"train\"],\n",
    "    eval_dataset=formatted_dataset[\"val\"],\n",
    "    # Removed formatting_func as we pre-formatted the dataset\n",
    "    peft_config=lora_config,\n",
    "    args=training_args,\n",
    "    # Removed tokenizer argument\n",
    ")\n",
    "trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a8d529",
   "metadata": {
    "id": "d2a8d529"
   },
   "source": [
    "### 9. Fine-tune the model\n",
    "Training typically finishes in a few minutes on a T4 due to the compact dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d22fd2c",
   "metadata": {
    "id": "2d22fd2c"
   },
   "outputs": [],
   "source": [
    "train_result = trainer.train()\n",
    "train_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e5f60d",
   "metadata": {},
   "source": [
    "#### Plot training loss curve\n",
    "Use this after training to confirm the optimizer is behaving as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c064bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve once training has been run\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_history = [\n",
    "    (entry.get(\"step\"), entry[\"loss\"])\n",
    "    for entry in trainer.state.log_history\n",
    "    if \"loss\" in entry and entry.get(\"loss\") is not None and entry.get(\"step\") is not None\n",
    "]\n",
    "\n",
    "if not loss_history:\n",
    "    print(\"No loss logs available yet. Run `trainer.train()` first.\")\n",
    "else:\n",
    "    steps, losses = zip(*loss_history, strict=False)\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.plot(steps, losses, marker=\"o\", linewidth=1)\n",
    "    ax.set_xlabel(\"Step\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Training Loss (per logged step)\")\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd3fc6b",
   "metadata": {
    "id": "fbd3fc6b"
   },
   "source": [
    "### 10. Quick validation sanity check\n",
    "Generate answers for a few validation samples to verify the adapter behaviour before exporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e7cf3e",
   "metadata": {
    "id": "f8e7cf3e"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "\n",
    "def preview_response(example: dict, max_new_tokens: int = 256) -> str:\n",
    "    user_text = build_user_message(\n",
    "        example[\"instruction\"], example.get(\"rag\"), example.get(\"contexts\")\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a scientific research assistant who answers with citations.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": user_text},\n",
    "    ]\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    encoded = tokenizer(\n",
    "        prompt_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    encoded = {key: value.to(model.device) for key, value in encoded.items()}\n",
    "    if encoded[\"input_ids\"].shape[-1] == tokenizer.model_max_length:\n",
    "        print(\n",
    "            \"Prompt truncated to fit model_max_length; consider tightening contexts if this occurs often.\"\n",
    "        )\n",
    "    gen_config = GenerationConfig(\n",
    "        do_sample=False,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(**encoded, generation_config=gen_config)\n",
    "    response_ids = generated[0, encoded[\"input_ids\"].shape[-1] :]\n",
    "    return tokenizer.decode(response_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "for example in dataset[\"val\"].select(range(min(3, len(dataset[\"val\"])))):\n",
    "    print(\"Instruction:\", example[\"instruction\"])\n",
    "    print(\"Ground truth:\", example.get(\"expected_response\", \"\").strip())\n",
    "    print(\"Model output:\", preview_response(example))\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf72b2",
   "metadata": {
    "id": "f5bf72b2"
   },
   "source": [
    "### 11. Save adapters and tokenizer\n",
    "Store artifacts under `outputs/adapters/lora_science_v1`. Upload to Drive or remote storage as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df38e321",
   "metadata": {
    "id": "df38e321"
   },
   "outputs": [],
   "source": [
    "adapter_dir = output_root / \"adapters\"\n",
    "adapter_dir.mkdir(parents=True, exist_ok=True)\n",
    "adapter_path = adapter_dir / \"lora_science_v1\"\n",
    "trainer.model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "print(f\"Saved LoRA adapter to {adapter_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa003a7",
   "metadata": {
    "id": "9fa003a7"
   },
   "source": [
    "### 12. Package artifacts\n",
    "Compress the adapter, training args, and logs for upload back to the repo or Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c60cb7",
   "metadata": {
    "id": "d9c60cb7"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "archive_stem = output_root.parent / \"lora_science_v1_artifacts\"\n",
    "zip_target = archive_stem.with_suffix(\".zip\")\n",
    "if zip_target.exists():\n",
    "    zip_target.unlink()\n",
    "zip_path = shutil.make_archive(str(archive_stem), \"zip\", root_dir=output_root)\n",
    "print(f\"Packed artifacts at {zip_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a76a332",
   "metadata": {
    "id": "6a76a332"
   },
   "source": [
    "### 13. Next steps\n",
    "1. **Persist training metadata** &rightarrow; run the next cell to capture seed `20251101`, the key hyperparameters, and recent metrics in `outputs/adapters/lora_science_v1/EXPERIMENT_METADATA.json`.\n",
    "2. **Materialize merged weights** &rightarrow; execute the following cell to merge the LoRA adapter into the base model and save a ready-to-quantize checkpoint under `outputs/lora_science_v1/merged_full_model`.\n",
    "3. **Quantize for Ollama** &rightarrow; use the provided CLI snippet to convert the merged weights to GGUF via `llama.cpp`'s `convert-hf-to-gguf.py`, then register the artifact with Ollama.\n",
    "4. **Re-benchmark** &rightarrow; rerun `scripts/evaluate_models.py` with the new Ollama tag to compare against `rag_baseline_v1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabcb563",
   "metadata": {
    "id": "dabcb563"
   },
   "outputs": [],
   "source": [
    "# Persist experiment metadata for reproducibility\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import peft\n",
    "import transformers\n",
    "import trl\n",
    "from packaging.version import Version\n",
    "\n",
    "metadata_target = output_root / \"adapters\" / \"lora_science_v1\" / \"EXPERIMENT_METADATA.json\"\n",
    "metadata_target.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "training_summary = {\n",
    "    \"seed\": 20251101,\n",
    "    \"timestamp_utc\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
    "    \"base_model\": model_name,\n",
    "    \"adapter_dir\": str(output_root / \"adapters\" / \"lora_science_v1\"),\n",
    "    \"output_dir\": str(output_root),\n",
    "    \"hyperparameters\": {\n",
    "        \"num_train_epochs\": training_args.num_train_epochs,\n",
    "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"weight_decay\": training_args.weight_decay,\n",
    "        \"lr_scheduler_type\": training_args.lr_scheduler_type,\n",
    "        \"warmup_ratio\": training_args.warmup_ratio,\n",
    "        \"max_seq_length\": 1024,\n",
    "    },\n",
    "    \"optimizer_steps\": trainer.state.global_step,\n",
    "    \"train_loss\": next(\n",
    "        (entry.get(\"loss\") for entry in reversed(trainer.state.log_history) if \"loss\" in entry),\n",
    "        None,\n",
    "    ),\n",
    "    \"final_metrics\": train_result.metrics\n",
    "    if \"train_result\" in locals() and train_result is not None\n",
    "    else {},\n",
    "    \"libraries\": {\n",
    "        \"transformers\": Version(transformers.__version__).base_version,\n",
    "        \"trl\": Version(trl.__version__).base_version,\n",
    "        \"peft\": Version(peft.__version__).base_version,\n",
    "    },\n",
    "}\n",
    "\n",
    "metadata_target.write_text(\n",
    "    json.dumps(training_summary, indent=2, sort_keys=True) + \"\\n\", encoding=\"utf-8\"\n",
    ")\n",
    "print(f\"Wrote metadata to {metadata_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c557f3",
   "metadata": {
    "id": "76c557f3"
   },
   "outputs": [],
   "source": [
    "# Merge the LoRA adapter back into the base model and save full weights\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "merged_output_dir = output_root / \"merged_full_model\"\n",
    "merged_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Merging adapter into the base model…\")\n",
    "merge_dtype = globals().get(\"model_dtype\", torch.float16)\n",
    "merged_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    adapter_path,\n",
    "    torch_dtype=merge_dtype,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ").merge_and_unload()\n",
    "\n",
    "merged_model.to(\"cpu\")\n",
    "merged_model.save_pretrained(merged_output_dir)\n",
    "tokenizer.save_pretrained(merged_output_dir)\n",
    "print(f\"Merged checkpoint written to {merged_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9361cf2",
   "metadata": {
    "id": "b9361cf2"
   },
   "source": [
    "#### Quantize and evaluate\n",
    "Run the commands below from your local checkout once the merged checkpoint is synced down:\n",
    "```bash\n",
    "# Convert merged HF weights to GGUF (example with Q4_0 quantization)\n",
    "python /path/to/llama.cpp/convert-hf-to-gguf.py \\\n",
    "  --model-dir outputs/lora_science_v1/merged_full_model \\\n",
    "  --outfile outputs/lora_science_v1/merged_full_model/Qwen2-0.5B-lora_science_v1.Q4_0.gguf \\\n",
    "  --data-type q4_0\n",
    "\n",
    "# Register a new Ollama model tag\n",
    "ollama create qwen2-lora-science -f ollama/Modelfile\n",
    "ollama push qwen2-lora-science\n",
    "\n",
    "# Re-run the offline evaluation with the new tag\n",
    "python scripts/evaluate_models.py \\\n",
    "  --model-tag qwen2-lora-science \\\n",
    "  --preset configs/retrieval_presets/qa_default.yaml \\\n",
    "  --output-dir evaluation/results/lora_science_v1```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12bc5be",
   "metadata": {
    "id": "e12bc5be"
   },
   "source": [
    "### 14. Re-package artifacts\n",
    "Create a fresh archive after merging and metadata updates so downstream steps stay in sync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c16601",
   "metadata": {
    "id": "d2c16601"
   },
   "outputs": [],
   "source": [
    "# Rebuild the archive to include merged weights and metadata\n",
    "import shutil\n",
    "\n",
    "archive_stem = output_root.parent / \"lora_science_v1_artifacts_postmerge\"\n",
    "zip_target = archive_stem.with_suffix(\".zip\")\n",
    "if zip_target.exists():\n",
    "    zip_target.unlink()\n",
    "zip_path = shutil.make_archive(str(archive_stem), \"zip\", root_dir=output_root)\n",
    "print(f\"Packed updated artifacts at {zip_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
