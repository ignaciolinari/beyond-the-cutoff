{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ede252e",
   "metadata": {
    "id": "9ede252e"
   },
   "source": [
    "# LoRA Fine-Tuning: lora_science_v1_instruction_only\n",
    "\n",
    "This Colab-ready workflow fine-tunes `Qwen/Qwen2.5-0.5B-Instruct` with LoRA adapters on the offline dataset **WITHOUT RAG contexts** for the 6-condition experiment.\n",
    "\n",
    "**Important**: This notebook trains the model WITHOUT RAG contexts (instruction only). This model is used for:\n",
    "- **Condition 3**: FT-only (evaluated without RAG contexts)\n",
    "- **Condition 4**: FT+RAG instruction-only (evaluated WITH RAG contexts)\n",
    "\n",
    "For conditions 5-6, use `lora_science_v1.ipynb` which trains WITH RAG contexts.\n",
    "\n",
    "> This model is designed to answer questions based on its knowledge alone. When evaluated with RAG contexts (condition 4), it tests transfer learning - whether a model trained without contexts can benefit from RAG at inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6edeb68",
   "metadata": {
    "id": "d6edeb68"
   },
   "source": [
    "### 0. Runtime checklist\n",
    "* Select a Colab runtime with GPU (T4 preferred) before running any code.\n",
    "* Keep an eye on VRAM usage (~16 GB on T4). Reduce sequence length or increase gradient accumulation if memory errors appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d0828",
   "metadata": {
    "id": "776d0828"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b9ca47",
   "metadata": {
    "id": "54b9ca47"
   },
   "source": [
    "### 1. Install Python dependencies\n",
    "We pin versions compatible with Transformers 4.45+ and TRL's SFTTrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa52980",
   "metadata": {
    "id": "2fa52980"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade pip\n",
    "!pip install --quiet \"transformers>=4.45.0\" \"accelerate>=0.33.0\" \"datasets>=3.0.0\" peft trl bitsandbytes sentencepiece evaluate huggingface_hub pynvml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b421ec84",
   "metadata": {
    "id": "b421ec84"
   },
   "source": [
    "### 2. Authenticate (optional but recommended)\n",
    "Set Hugging Face and GitHub tokens if you plan to pull private assets or push adapters. Tokens are stored in-memory only for this runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b309bd8",
   "metadata": {
    "id": "7b309bd8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from getpass import getpass\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token is None:\n",
    "    entered = getpass(\"Enter Hugging Face token (leave blank to skip): \")\n",
    "    hf_token = entered.strip() or None\n",
    "if hf_token:\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "    subprocess.run(\n",
    "        [\"huggingface-cli\", \"login\", \"--token\", hf_token, \"--add-to-git-credential\"], check=False\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping Hugging Face login.\")\n",
    "\n",
    "if os.environ.get(\"GITHUB_TOKEN\") is None:\n",
    "    gh_token = getpass(\"Enter GitHub token for private repo access (leave blank to skip): \")\n",
    "    if gh_token.strip():\n",
    "        os.environ[\"GITHUB_TOKEN\"] = gh_token.strip()\n",
    "        print(\"Stored GitHub token in this session.\")\n",
    "    else:\n",
    "        print(\"Skipping GitHub token setup. Upload the dataset manually if download fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159b6149",
   "metadata": {
    "id": "159b6149"
   },
   "source": [
    "### 3. (Optional) Mount Google Drive\n",
    "Use Drive if you want automatic persistence of adapters, logs, or dataset snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807063f",
   "metadata": {
    "id": "a807063f"
   },
   "outputs": [],
   "source": [
    "USE_DRIVE = True\n",
    "if USE_DRIVE:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "    BASE_DIR = \"/content/drive/MyDrive/beyond-the-cutoff\"\n",
    "else:\n",
    "    BASE_DIR = \"/content\"\n",
    "print(f\"Working directory base: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9492ed8",
   "metadata": {
    "id": "e9492ed8"
   },
   "source": [
    "### 4. Retrieve the training dataset\n",
    "We use `train_dataset.jsonl` which contains only the training portion of the data. The evaluation portion (`eval_dataset.jsonl`) is held out for the final experiment.\n",
    "\n",
    "**Important**: Use `train_dataset.jsonl`, NOT `offline_dataset.jsonl`. This prevents data leakage — the model won't see the exact questions used in the final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5d8c9",
   "metadata": {
    "id": "46b5d8c9"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(BASE_DIR) / \"data\" / \"offline_eval\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATASET_PATH = DATA_DIR / \"train_dataset.jsonl\"\n",
    "\n",
    "if DATASET_PATH.exists():\n",
    "    print(f\"Dataset already available: {DATASET_PATH}\")\n",
    "else:\n",
    "    import requests\n",
    "\n",
    "    headers = {}\n",
    "    github_token = os.environ.get(\"GITHUB_TOKEN\")\n",
    "    if github_token:\n",
    "        headers[\"Authorization\"] = f\"token {github_token}\"\n",
    "    # Use train_dataset.jsonl (not offline_dataset.jsonl) to prevent data leakage\n",
    "    url = \"https://raw.githubusercontent.com/ignaciolinari/beyond-the-cutoff/main/evaluation/datasets/train_dataset.jsonl\"\n",
    "    response = requests.get(url, headers=headers, timeout=60)\n",
    "    if response.status_code == 200:\n",
    "        DATASET_PATH.write_text(response.text, encoding=\"utf-8\")\n",
    "        print(f\"Downloaded training dataset to {DATASET_PATH}\")\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed to download dataset (status {response.status_code}). \"\n",
    "            \"Upload train_dataset.jsonl manually and rerun this cell.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a001ed2",
   "metadata": {
    "id": "7a001ed2"
   },
   "source": [
    "### 5. Create deterministic train/val split for training\n",
    "We stratify by paper and task type using seed `20251101`. This internal split is for **training validation only** — the final experiment evaluation uses the separate `eval_dataset.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d6a30",
   "metadata": {
    "id": "f95d6a30"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def load_examples(path: Path) -> list[dict]:\n",
    "    raw = path.read_text(encoding=\"utf-8\").strip().splitlines()\n",
    "    return [json.loads(line) for line in raw if line]\n",
    "\n",
    "\n",
    "def extract_paper_id(example: dict) -> str:\n",
    "    meta = example.get(\"metadata\") or {}\n",
    "    if isinstance(meta, dict):\n",
    "        candidate = meta.get(\"source_path\") or meta.get(\"paper_id\")\n",
    "        if candidate:\n",
    "            return Path(str(candidate)).stem\n",
    "    sources = example.get(\"sources\") or []\n",
    "    if sources:\n",
    "        return Path(str(sources[0])).stem\n",
    "    rag = example.get(\"rag\") or {}\n",
    "    retrieved = rag.get(\"retrieved\") or []\n",
    "    if retrieved:\n",
    "        first = retrieved[0]\n",
    "        if isinstance(first, dict) and first.get(\"source_path\"):\n",
    "            return Path(str(first[\"source_path\"])).stem\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "examples = load_examples(DATASET_PATH)\n",
    "print(f\"Loaded {len(examples)} training examples\")\n",
    "\n",
    "groups = defaultdict(list)\n",
    "for example in examples:\n",
    "    key = (extract_paper_id(example), example.get(\"task_type\"))\n",
    "    groups[key].append(example)\n",
    "\n",
    "buckets = list(groups.values())\n",
    "rng = random.Random(20251101)\n",
    "rng.shuffle(buckets)\n",
    "\n",
    "# 85/15 split for train/val (internal training validation only)\n",
    "# The held-out eval_dataset.jsonl is used for final experiment evaluation\n",
    "target_counts = {\n",
    "    \"train\": int(round(0.85 * len(examples))),\n",
    "    \"val\": int(round(0.15 * len(examples))),\n",
    "}\n",
    "splits = {\"train\": [], \"val\": []}\n",
    "\n",
    "for bucket in buckets:\n",
    "    remaining = {split: target_counts[split] - len(splits[split]) for split in splits}\n",
    "    target_split = max(remaining, key=lambda split: remaining[split])\n",
    "    if remaining[target_split] <= 0:\n",
    "        target_split = \"train\"\n",
    "    splits[target_split].extend(bucket)\n",
    "\n",
    "SPLIT_DIR = DATA_DIR / \"splits\"\n",
    "SPLIT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "for split, rows in splits.items():\n",
    "    out_path = SPLIT_DIR / f\"lora_science_v1_instruction_only_{split}.jsonl\"\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as handle:\n",
    "        for row in rows:\n",
    "            handle.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"{split:>5}: {len(rows):>3} examples → {out_path}\")\n",
    "\n",
    "print(\"\\nNote: Final evaluation uses eval_dataset.jsonl (not part of this training data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10890a97",
   "metadata": {
    "id": "10890a97"
   },
   "source": [
    "### 6. Load dataset into Hugging Face `DatasetDict`\n",
    "We keep the raw fields and delegate prompt construction to the trainer’s formatting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68899475",
   "metadata": {
    "id": "68899475"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "def read_split(split: str) -> Dataset:\n",
    "    path = SPLIT_DIR / f\"lora_science_v1_instruction_only_{split}.jsonl\"\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as handle:\n",
    "        for line in handle:\n",
    "            if line.strip():\n",
    "                rows.append(json.loads(line))\n",
    "    return Dataset.from_list(rows)\n",
    "\n",
    "\n",
    "dataset = DatasetDict({split: read_split(split) for split in [\"train\", \"val\"]})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d280fb3",
   "metadata": {
    "id": "5d280fb3"
   },
   "source": [
    "### 7. Initialize tokenizer, model, and LoRA config\n",
    "We keep the base model in float16 and target attention + MLP projections for LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e94e923",
   "metadata": {
    "id": "5e94e923"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "supports_cuda = torch.cuda.is_available()\n",
    "\n",
    "compute_capability = torch.cuda.get_device_capability(0)[0] if supports_cuda else None\n",
    "\n",
    "prefer_bf16 = bool(supports_cuda and compute_capability is not None and compute_capability >= 8)\n",
    "\n",
    "model_dtype = torch.bfloat16 if prefer_bf16 else torch.float16\n",
    "\n",
    "\n",
    "def build_base_model() -> AutoModelForCausalLM:\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=model_dtype,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    base_model.config.use_cache = False\n",
    "    return base_model\n",
    "\n",
    "\n",
    "model = build_base_model()\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(f\"Loaded model on {model.device} with dtype {model_dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09821262",
   "metadata": {
    "id": "09821262"
   },
   "source": [
    "### 8. Define prompt formatting and trainer\n",
    "**INSTRUCTION-ONLY MODE**: We use only the instruction (no RAG contexts) in the user turn. This trains the model to answer questions based on its knowledge alone, matching the FT-only evaluation setup.\n",
    "\n",
    "**Important**: The user message format matches `_build_instruction_only_prompt()` from the evaluation runner to ensure training/evaluation consistency. The prompt includes the instruction text wrapped in the same format used during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc1c52",
   "metadata": {
    "id": "14dc1c52"
   },
   "outputs": [],
   "source": [
    "from collections.abc import Mapping, Sequence\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "\n",
    "def _get_batch_value(column, index):\n",
    "    if column is None:\n",
    "        return None\n",
    "    if isinstance(column, Mapping):\n",
    "        return {key: _get_batch_value(value, index) for key, value in column.items()}\n",
    "    if isinstance(column, Sequence) and not isinstance(column, str | bytes):\n",
    "        return column[index] if len(column) > index else None\n",
    "    return column\n",
    "\n",
    "\n",
    "def build_user_message(\n",
    "    instruction: str,\n",
    "    rag_entry: dict | None,\n",
    "    contexts_fallback: Sequence[str] | None,\n",
    ") -> str:\n",
    "    # INSTRUCTION-ONLY MODE: Return only instruction, ignore contexts\n",
    "    # This trains the model to answer without RAG contexts\n",
    "    # Format matches evaluation runner's _build_instruction_only_prompt() for consistency\n",
    "    # System message comes from Modelfile, so user content should NOT duplicate it\n",
    "    instruction_text = instruction.strip()\n",
    "    if not instruction_text:\n",
    "        return \"\"\n",
    "    return f\"Question: {instruction_text}\\n\\nAnswer:\"\n",
    "\n",
    "\n",
    "def format_example(example: dict) -> dict[str, str]:\n",
    "    instruction = (example[\"instruction\"] or \"\").strip()\n",
    "    rag_item = example.get(\"rag\")\n",
    "    contexts_item = example.get(\"contexts\")\n",
    "    user_content = build_user_message(instruction, rag_item, contexts_item)\n",
    "    assistant_reply = (example.get(\"expected_response\") or \"\").strip()\n",
    "    # Use chat template format (required for HuggingFace models)\n",
    "    # but user content matches evaluation format for consistency\n",
    "    # System message is minimal since instructions are in user content\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a research paper assistant.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_reply},\n",
    "    ]\n",
    "    rendered = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    # Ensure EOS token is added here as packing is False\n",
    "    if tokenizer.eos_token:\n",
    "        rendered += tokenizer.eos_token\n",
    "    return {\"text\": rendered}\n",
    "\n",
    "\n",
    "output_root = Path(BASE_DIR) / \"outputs\" / \"lora_science_v1_instruction_only\"\n",
    "output_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "model_dtype = globals().get(\"model_dtype\", getattr(model, \"dtype\", torch.float16))\n",
    "supports_cuda = torch.cuda.is_available()\n",
    "prefer_bf16 = globals().get(\"prefer_bf16\", supports_cuda and model_dtype == torch.bfloat16)\n",
    "fp16_flag = supports_cuda and model_dtype == torch.float16\n",
    "bf16_flag = supports_cuda and prefer_bf16\n",
    "tokenizer.model_max_length = 1024\n",
    "\n",
    "\n",
    "# Ensure tokenizer.eos_token is a string\n",
    "if tokenizer.eos_token is None or not isinstance(tokenizer.eos_token, str):\n",
    "    tokenizer.eos_token = \"<|endoftext|>\"  # Set a common EOS token if it's not already a string\n",
    "\n",
    "\n",
    "# Make sure we do not stack multiple adapters on the same model instance\n",
    "if hasattr(model, \"peft_config\"):\n",
    "    model = build_base_model()\n",
    "\n",
    "\n",
    "# Apply formatting to the dataset before passing to SFTTrainer\n",
    "formatted_dataset = dataset.map(format_example, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "\n",
    "# Deduplicate formatted prompts to ensure no duplicate prompts in training\n",
    "# This prevents data leakage and ensures evaluation can use the same config\n",
    "def deduplicate_dataset(ds, split_name: str):\n",
    "    \"\"\"Remove duplicate prompts based on formatted text.\"\"\"\n",
    "    seen_prompts = set()\n",
    "    unique_indices = []\n",
    "    duplicates_count = 0\n",
    "\n",
    "    for idx, example in enumerate(ds):\n",
    "        prompt_text = example.get(\"text\", \"\").strip()\n",
    "        if prompt_text and prompt_text not in seen_prompts:\n",
    "            seen_prompts.add(prompt_text)\n",
    "            unique_indices.append(idx)\n",
    "        else:\n",
    "            duplicates_count += 1\n",
    "\n",
    "    if duplicates_count > 0:\n",
    "        print(f\"[info] Removed {duplicates_count} duplicate prompt(s) from {split_name} split\")\n",
    "\n",
    "    return ds.select(unique_indices) if unique_indices else ds\n",
    "\n",
    "\n",
    "formatted_dataset[\"train\"] = deduplicate_dataset(formatted_dataset[\"train\"], \"train\")\n",
    "formatted_dataset[\"val\"] = deduplicate_dataset(formatted_dataset[\"val\"], \"val\")\n",
    "\n",
    "print(\n",
    "    f\"After deduplication: train={len(formatted_dataset['train'])}, val={len(formatted_dataset['val'])}\"\n",
    ")\n",
    "\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=str(output_root),\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=fp16_flag,\n",
    "    bf16=bf16_flag,\n",
    "    max_grad_norm=1.0,\n",
    "    report_to=\"none\",\n",
    "    packing=False,  # Keep packing=False as we formatted the text ourselves\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset[\"train\"],\n",
    "    eval_dataset=formatted_dataset[\"val\"],\n",
    "    # Removed formatting_func as we pre-formatted the dataset\n",
    "    peft_config=lora_config,\n",
    "    args=training_args,\n",
    "    # Removed tokenizer argument\n",
    ")\n",
    "trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a8d529",
   "metadata": {
    "id": "d2a8d529"
   },
   "source": [
    "### 9. Fine-tune the model\n",
    "Training time will depend on the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d22fd2c",
   "metadata": {
    "id": "2d22fd2c"
   },
   "outputs": [],
   "source": [
    "train_batch_size = training_args.per_device_train_batch_size * max(\n",
    "    1, training_args.gradient_accumulation_steps\n",
    ")\n",
    "print(\n",
    "    f\"Starting LoRA fine-tuning on {len(formatted_dataset['train'])} training examples \"\n",
    "    f\"({train_batch_size} effective batch size) for {training_args.num_train_epochs} epochs.\"\n",
    ")\n",
    "print(\n",
    "    f\"Evaluation enabled: {training_args.eval_strategy != 'no'} | \"\n",
    "    f\"Logging every {training_args.logging_steps} steps | \"\n",
    "    f\"Gradient checkpointing: {training_args.gradient_checkpointing}\"\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(\"Training finished.\")\n",
    "trainer.log_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "if train_result.metrics:\n",
    "    print(\"Train metrics:\")\n",
    "    for key, value in train_result.metrics.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "eval_metrics = None\n",
    "if training_args.eval_strategy != \"no\":\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    trainer.log_metrics(\"eval\", eval_metrics)\n",
    "    trainer.save_metrics(\"eval\", eval_metrics)\n",
    "    if eval_metrics:\n",
    "        print(\"Eval metrics:\")\n",
    "        for key, value in eval_metrics.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"Evaluation skipped (eval_strategy='no').\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e5f60d",
   "metadata": {},
   "source": [
    "#### Plot training loss curve\n",
    "Use this after training to confirm the optimizer is behaving as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c064bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "loss_history = [\n",
    "    {\"step\": entry[\"step\"], \"loss\": entry[\"loss\"]}\n",
    "    for entry in trainer.state.log_history\n",
    "    if \"loss\" in entry and \"step\" in entry\n",
    "]\n",
    "\n",
    "if not loss_history:\n",
    "    print(\"No loss values logged yet. Run the training cell first or raise logging verbosity.\")\n",
    "else:\n",
    "    loss_df = pd.DataFrame(loss_history).drop_duplicates(subset=\"step\").sort_values(\"step\")\n",
    "    display(loss_df.tail())\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "    ax.plot(loss_df[\"step\"], loss_df[\"loss\"], marker=\"o\", linewidth=1.5, markersize=3)\n",
    "    ax.set_xlabel(\"Global step\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Training Loss vs. Step\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd3fc6b",
   "metadata": {
    "id": "fbd3fc6b"
   },
   "source": [
    "### 10. Quick validation sanity check\n",
    "Generate answers for a few validation samples to verify the adapter behaviour before exporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e7cf3e",
   "metadata": {
    "id": "f8e7cf3e"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "\n",
    "def preview_response(example: dict, max_new_tokens: int = 256) -> str:\n",
    "    user_text = build_user_message(\n",
    "        example[\"instruction\"], example.get(\"rag\"), example.get(\"contexts\")\n",
    "    )\n",
    "    # Match training format: minimal system message, user content includes instructions\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a research paper assistant.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": user_text},\n",
    "    ]\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    encoded = tokenizer(\n",
    "        prompt_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    encoded = {key: value.to(model.device) for key, value in encoded.items()}\n",
    "    if encoded[\"input_ids\"].shape[-1] == tokenizer.model_max_length:\n",
    "        print(\n",
    "            \"Prompt truncated to fit model_max_length; consider tightening contexts if this occurs often.\"\n",
    "        )\n",
    "    gen_config = GenerationConfig(\n",
    "        do_sample=False,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(**encoded, generation_config=gen_config)\n",
    "    response_ids = generated[0, encoded[\"input_ids\"].shape[-1] :]\n",
    "    return tokenizer.decode(response_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "for example in dataset[\"val\"].select(range(min(3, len(dataset[\"val\"])))):\n",
    "    print(\"Instruction:\", example[\"instruction\"])\n",
    "    print(\"Ground truth:\", example.get(\"expected_response\", \"\").strip())\n",
    "    print(\"Model output:\", preview_response(example))\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf72b2",
   "metadata": {
    "id": "f5bf72b2"
   },
   "source": [
    "### 11. Save adapters and tokenizer\n",
    "Store artifacts under `outputs/adapters/lora_science_v1_instruction_only`. Upload to Drive or remote storage as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df38e321",
   "metadata": {
    "id": "df38e321"
   },
   "outputs": [],
   "source": [
    "adapter_dir = output_root / \"adapters\"\n",
    "adapter_dir.mkdir(parents=True, exist_ok=True)\n",
    "adapter_path = adapter_dir / \"lora_science_v1_instruction_only\"\n",
    "trainer.model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "print(f\"Saved LoRA adapter to {adapter_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa003a7",
   "metadata": {
    "id": "9fa003a7"
   },
   "source": [
    "### 12. Package artifacts\n",
    "Compress the adapter, training args, and logs for upload back to the repo or Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c60cb7",
   "metadata": {
    "id": "d9c60cb7"
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# archive_stem = output_root.parent / \"lora_science_v1_artifacts\"\n",
    "# zip_target = archive_stem.with_suffix(\".zip\")\n",
    "# if zip_target.exists():\n",
    "#     zip_target.unlink()\n",
    "# zip_path = shutil.make_archive(str(archive_stem), \"zip\", root_dir=output_root)\n",
    "# print(f\"Packed artifacts at {zip_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a76a332",
   "metadata": {
    "id": "6a76a332"
   },
   "source": [
    "### 13. Next steps\n",
    "1. **Persist training metadata** &rightarrow; run the next cell to capture seed `20251101`, the key hyperparameters, and recent metrics in `outputs/adapters/lora_science_v1_instruction_only/EXPERIMENT_METADATA.json`.\n",
    "2. **Materialize merged weights** &rightarrow; execute the following cell to merge the LoRA adapter into the base model and save a ready-to-quantize checkpoint under `outputs/lora_science_v1_instruction_only/merged_full_model`.\n",
    "3. **Quantize for Ollama** &rightarrow; use the provided CLI snippet to convert the merged weights to GGUF via `llama.cpp`'s `convert-hf-to-gguf.py`, then register the artifact with Ollama.\n",
    "4. **Re-benchmark** &rightarrow; use this model for both FT-only evaluation (condition 3) and FT+RAG instruction-only evaluation (condition 4). For conditions 5-6, use the RAG-trained model from `lora_science_v1.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabcb563",
   "metadata": {
    "id": "dabcb563"
   },
   "outputs": [],
   "source": [
    "# Persist experiment metadata for reproducibility\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import peft\n",
    "import transformers\n",
    "import trl\n",
    "from packaging.version import Version\n",
    "\n",
    "metadata_target = (\n",
    "    output_root / \"adapters\" / \"lora_science_v1_instruction_only\" / \"EXPERIMENT_METADATA.json\"\n",
    ")\n",
    "metadata_target.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "training_summary = {\n",
    "    \"seed\": 20251101,\n",
    "    \"timestamp_utc\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
    "    \"base_model\": model_name,\n",
    "    \"training_mode\": \"instruction_only\",  # Mark this as instruction-only training\n",
    "    \"adapter_dir\": str(output_root / \"adapters\" / \"lora_science_v1_instruction_only\"),\n",
    "    \"output_dir\": str(output_root),\n",
    "    \"hyperparameters\": {\n",
    "        \"num_train_epochs\": training_args.num_train_epochs,\n",
    "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"weight_decay\": training_args.weight_decay,\n",
    "        \"lr_scheduler_type\": training_args.lr_scheduler_type,\n",
    "        \"warmup_ratio\": training_args.warmup_ratio,\n",
    "        \"max_seq_length\": 1024,\n",
    "    },\n",
    "    \"optimizer_steps\": trainer.state.global_step,\n",
    "    \"train_loss\": next(\n",
    "        (entry.get(\"loss\") for entry in reversed(trainer.state.log_history) if \"loss\" in entry),\n",
    "        None,\n",
    "    ),\n",
    "    \"final_metrics\": train_result.metrics\n",
    "    if \"train_result\" in locals() and train_result is not None\n",
    "    else {},\n",
    "    \"libraries\": {\n",
    "        \"transformers\": Version(transformers.__version__).base_version,\n",
    "        \"trl\": Version(trl.__version__).base_version,\n",
    "        \"peft\": Version(peft.__version__).base_version,\n",
    "    },\n",
    "}\n",
    "\n",
    "metadata_target.write_text(\n",
    "    json.dumps(training_summary, indent=2, sort_keys=True) + \"\\n\", encoding=\"utf-8\"\n",
    ")\n",
    "print(f\"Wrote metadata to {metadata_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c557f3",
   "metadata": {
    "id": "76c557f3"
   },
   "outputs": [],
   "source": [
    "# Merge the LoRA adapter back into the base model and save full weights\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "merged_output_dir = output_root / \"merged_full_model\"\n",
    "merged_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Merging adapter into the base model…\")\n",
    "merge_dtype = globals().get(\"model_dtype\", torch.float16)\n",
    "merged_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    adapter_path,\n",
    "    torch_dtype=merge_dtype,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ").merge_and_unload()\n",
    "\n",
    "merged_model.to(\"cpu\")\n",
    "merged_model.save_pretrained(merged_output_dir)\n",
    "tokenizer.save_pretrained(merged_output_dir)\n",
    "print(f\"Merged checkpoint written to {merged_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9361cf2",
   "metadata": {
    "id": "b9361cf2"
   },
   "source": [
    "#### Quantize and evaluate\n",
    "Run the commands below from your local checkout once the merged checkpoint is synced down:\n",
    "```bash\n",
    "# Convert merged HF weights to GGUF (using Q4_K_M quantization to match Ollama)\n",
    "python /path/to/llama.cpp/convert-hf-to-gguf.py \\\n",
    "  --model-dir outputs/lora_science_v1_instruction_only/merged_full_model \\\n",
    "  --outfile outputs/lora_science_v1_instruction_only/merged_full_model/Qwen2.5-0.5B-lora_science_v1_instruction_only.Q4_K_M.gguf \\\n",
    "  --data-type q4_K_M\n",
    "\n",
    "# Register a new Ollama model tag for instruction-only model\n",
    "# Use Modelfile.instruction_only which has the correct system prompt (no citations)\n",
    "ollama create lora_science_0p5_instruction_only -f ollama/Modelfile.instruction_only\n",
    "ollama push lora_science_0p5_instruction_only\n",
    "\n",
    "# Run FT-only evaluation (instruction-only prompts)\n",
    "python scripts/evaluate_models.py \\\n",
    "  --model-config configs/lora_science_v1_instruction_only_ollama.yaml \\\n",
    "  --model-label lora_science_0p5b_ft_only \\\n",
    "  --prompt-mode instruction \\\n",
    "  --output evaluation/results/lora_science_0p5b_ft_only/metrics.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12bc5be",
   "metadata": {
    "id": "e12bc5be"
   },
   "source": [
    "### 14. Re-package artifacts\n",
    "Create a fresh archive after merging and metadata updates so downstream steps stay in sync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c16601",
   "metadata": {
    "id": "d2c16601"
   },
   "outputs": [],
   "source": [
    "# Rebuild the archive to include merged weights and metadata\n",
    "import shutil\n",
    "\n",
    "archive_stem = output_root.parent / \"lora_science_v1_instruction_only_artifacts_postmerge\"\n",
    "zip_target = archive_stem.with_suffix(\".zip\")\n",
    "if zip_target.exists():\n",
    "    zip_target.unlink()\n",
    "zip_path = shutil.make_archive(str(archive_stem), \"zip\", root_dir=output_root)\n",
    "print(f\"Packed updated artifacts at {zip_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
